{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Generate Candidates\n",
    "\n",
    "Our goal during this step is to generate candidate mids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import get_connection \n",
    "from lib.utils import FB2M_NAME_TABLE\n",
    "\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8840370a84442d84b6ab9fac55c9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_index</th>\n",
       "      <th>object</th>\n",
       "      <th>predicted_question_tokens</th>\n",
       "      <th>predicted_subject_names</th>\n",
       "      <th>question</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>relation</th>\n",
       "      <th>start_index</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_name</th>\n",
       "      <th>subject_name_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17188</th>\n",
       "      <td>7.0</td>\n",
       "      <td>075s73</td>\n",
       "      <td>[which, town, is, in, new, york, city]</td>\n",
       "      <td>[{'name': 'new york city', 'score': tensor(64....</td>\n",
       "      <td>which town is in new york city</td>\n",
       "      <td>[which, town, is, in, new, york, city]</td>\n",
       "      <td>location/place_with_neighborhoods/neighborhoods</td>\n",
       "      <td>4.0</td>\n",
       "      <td>02_286</td>\n",
       "      <td>new york city</td>\n",
       "      <td>(new, york, city)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>4.0</td>\n",
       "      <td>017drs</td>\n",
       "      <td>[does, pee, wee, reese, play, shortstop, or, p...</td>\n",
       "      <td>[{'name': 'pee wee reese', 'score': tensor(91....</td>\n",
       "      <td>does pee wee reese play shortstop or power for...</td>\n",
       "      <td>[does, pee, wee, reese, play, shortstop, or, p...</td>\n",
       "      <td>baseball/baseball_player/position_s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01bg1k</td>\n",
       "      <td>pee wee reese</td>\n",
       "      <td>(pee, wee, reese)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21187</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0vp3fq</td>\n",
       "      <td>[what, is, a, track, by, lutricia, mcneal, ?]</td>\n",
       "      <td>[{'name': 'lutricia mcneal', 'score': tensor(7...</td>\n",
       "      <td>What is a track by lutricia mcneal?</td>\n",
       "      <td>[what, is, a, track, by, lutricia, mcneal, ?]</td>\n",
       "      <td>music/artist/track</td>\n",
       "      <td>5.0</td>\n",
       "      <td>06tw28</td>\n",
       "      <td>lutricia  mcneal</td>\n",
       "      <td>(lutricia, mcneal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18730</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0wzyx1</td>\n",
       "      <td>[name, a, recording, by, nelson, mandela]</td>\n",
       "      <td>[{'name': 'nelson mandela', 'score': tensor(57...</td>\n",
       "      <td>Name a recording by nelson mandela</td>\n",
       "      <td>[name, a, recording, by, nelson, mandela]</td>\n",
       "      <td>music/release_track/recording</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0slws_1</td>\n",
       "      <td>nelson mandela</td>\n",
       "      <td>(nelson, mandela)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0crryw4</td>\n",
       "      <td>[name, a, film, in, the, netflix, genre, celti...</td>\n",
       "      <td>[{'name': 'celtic music', 'score': tensor(106....</td>\n",
       "      <td>Name a film in the netflix genre celtic music.</td>\n",
       "      <td>[name, a, film, in, the, netflix, genre, celti...</td>\n",
       "      <td>media_common/netflix_genre/titles</td>\n",
       "      <td>7.0</td>\n",
       "      <td>01m1y</td>\n",
       "      <td>celtic music</td>\n",
       "      <td>(celtic, music)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       end_index   object                          predicted_question_tokens  \\\n",
       "17188        7.0   075s73             [which, town, is, in, new, york, city]   \n",
       "4793         4.0   017drs  [does, pee, wee, reese, play, shortstop, or, p...   \n",
       "21187        7.0   0vp3fq      [what, is, a, track, by, lutricia, mcneal, ?]   \n",
       "18730        6.0   0wzyx1          [name, a, recording, by, nelson, mandela]   \n",
       "10014        9.0  0crryw4  [name, a, film, in, the, netflix, genre, celti...   \n",
       "\n",
       "                                 predicted_subject_names  \\\n",
       "17188  [{'name': 'new york city', 'score': tensor(64....   \n",
       "4793   [{'name': 'pee wee reese', 'score': tensor(91....   \n",
       "21187  [{'name': 'lutricia mcneal', 'score': tensor(7...   \n",
       "18730  [{'name': 'nelson mandela', 'score': tensor(57...   \n",
       "10014  [{'name': 'celtic music', 'score': tensor(106....   \n",
       "\n",
       "                                                question  \\\n",
       "17188                     which town is in new york city   \n",
       "4793   does pee wee reese play shortstop or power for...   \n",
       "21187                What is a track by lutricia mcneal?   \n",
       "18730                 Name a recording by nelson mandela   \n",
       "10014     Name a film in the netflix genre celtic music.   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "17188             [which, town, is, in, new, york, city]   \n",
       "4793   [does, pee, wee, reese, play, shortstop, or, p...   \n",
       "21187      [what, is, a, track, by, lutricia, mcneal, ?]   \n",
       "18730          [name, a, recording, by, nelson, mandela]   \n",
       "10014  [name, a, film, in, the, netflix, genre, celti...   \n",
       "\n",
       "                                              relation  start_index  subject  \\\n",
       "17188  location/place_with_neighborhoods/neighborhoods          4.0   02_286   \n",
       "4793               baseball/baseball_player/position_s          1.0   01bg1k   \n",
       "21187                               music/artist/track          5.0   06tw28   \n",
       "18730                    music/release_track/recording          4.0  0slws_1   \n",
       "10014                media_common/netflix_genre/titles          7.0    01m1y   \n",
       "\n",
       "           subject_name subject_name_tokens  \n",
       "17188     new york city   (new, york, city)  \n",
       "4793      pee wee reese   (pee, wee, reese)  \n",
       "21187  lutricia  mcneal  (lutricia, mcneal)  \n",
       "18730    nelson mandela   (nelson, mandela)  \n",
       "10014      celtic music     (celtic, music)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "df = pd.read_pickle('step_1_predict_subject_name.pkl')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define text preprocessing the same as the training data and step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../..\\notebooks\\Simple QA Models\\Subject Recognition Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import lib.import_notebook\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "PREPROCESS = importlib.import_module(\n",
    "                \"notebooks.Simple QA Models.Subject Recognition Data\").preprocess\n",
    "TOKENIZE = importlib.import_module(\n",
    "                \"notebooks.Simple QA Models.Subject Recognition Data\").spacy_tokenize\n",
    "\n",
    "def text_preprocess(s):\n",
    "    # Define `text_preprocess` the way the input text was preprocessed before step 1\n",
    "    s = PREPROCESS(s)\n",
    "    s = TOKENIZE(s)\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "def text_normalize_punctuation(s):\n",
    "    s = text_preprocess(s)\n",
    "    # In `Normalized Reference Resolution#HYPOTHESIS - Subject Name not in Question.ipynb` we found that\n",
    "    # aliases and questions match up more if punctuation is removed.\n",
    "    # Remove punctuation\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    # Removing characters can create gaps of multiple spaces\n",
    "    # Substitue multiple spaces with one\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def text_normalize_punctuation_stem(s):\n",
    "    s = text_preprocess(s)\n",
    "    \n",
    "    # Remove Possessives\n",
    "    tokens = s.split()\n",
    "    possessives = [\"'s\"]\n",
    "    tokens = [t for t in tokens if t not in possessives]\n",
    "    # Stem\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    s = ' '.join(tokens)\n",
    "    \n",
    "    s = text_normalize_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Subject Aliases\n",
    "\n",
    "Create an index of subject aliases that are preprocessed similar to the predicted subect name. Allowing for a database lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_normalized_punctuation varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_normalized_punctuation_stem varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_preprocessed varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import psycopg2\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "def update_chunk(rows):\n",
    "    query = ('UPDATE ' + FB2M_NAME_TABLE + ' SET alias_preprocessed = %s, ' +\n",
    "             'alias_normalized_punctuation = %s, alias_normalized_punctuation_stem = %s ' +\n",
    "             'WHERE mid = %s and alias = %s')\n",
    "    psycopg2.extras.execute_batch(cursor, query, rows)\n",
    "\n",
    "cursor.execute('SELECT mid, alias FROM ' + FB2M_NAME_TABLE)\n",
    "rows = []\n",
    "for mid, alias in tqdm_notebook(cursor.fetchall()):\n",
    "    alias_preprocessed = text_preprocess(alias)\n",
    "    alias_normalized_punctuation = text_normalize_punctuation(alias)\n",
    "    alias_normalized_punctuation_stem = text_normalize_punctuation_stem(alias)\n",
    "    rows.append(tuple([alias_preprocessed, alias_normalized_punctuation, \n",
    "                       alias_normalized_punctuation_stem, mid, alias]))\n",
    "    \n",
    "    # Insert Chunk\n",
    "    if len(rows) > chunk_size:\n",
    "        update_chunk(rows)\n",
    "        rows = []\n",
    "        \n",
    "update_chunk(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_preprocessed ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_preprocessed);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_normalized_punctuation);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_normalized_punctuation_stem);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedObject",
     "evalue": "FEHLER:  Operatorklasse »gist_trgm_ops« existiert nicht für Zugriffsmethode »gist«\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedObject\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-08c16e5a552e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem_trgm ON ' + \n\u001b[1;32m----> 2\u001b[1;33m                FB2M_NAME_TABLE + ' USING gist(alias_normalized_punctuation_stem gist_trgm_ops);')\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUndefinedObject\u001b[0m: FEHLER:  Operatorklasse »gist_trgm_ops« existiert nicht für Zugriffsmethode »gist«\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem_trgm ON ' + \n",
    "               FB2M_NAME_TABLE + ' USING gist(alias_normalized_punctuation_stem gist_trgm_ops);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If subject name is null, then the question does not refer to the true alias. The example is then unanswerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answerable = df[df.subject_name.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics used to evaluate different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_candidates(candidates_mids):\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "    expected_accuracy = 0\n",
    "    n_answerable_examples = df_answerable.shape[0]\n",
    "    n_examples = df.shape[0]\n",
    "\n",
    "    for i, (_, row) in enumerate(df_answerable.iterrows()):\n",
    "        mids = candidates_mids[i]\n",
    "        if len(mids) == 0:\n",
    "            skipped += 1\n",
    "        elif row['subject'] in mids:\n",
    "            correct += 1\n",
    "            expected_accuracy += 1 / len(mids)\n",
    "        \n",
    "    print('Answerable Precision: %f [%d of %d]' %\n",
    "              (correct / (n_answerable_examples - skipped), correct,\n",
    "               (n_answerable_examples - skipped)))\n",
    "    print('Answerable Recall: %f [%d of %d]' %\n",
    "              ((n_answerable_examples - skipped) / n_answerable_examples,\n",
    "               (n_answerable_examples - skipped), n_answerable_examples))\n",
    "    print('Expected Guessing Accuracy: %f [%d of %d]' % \n",
    "              (expected_accuracy / n_examples, expected_accuracy, n_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic helper functions to run experiments quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_to_mid(text):\n",
    "    cursor.execute(\"SELECT mid FROM \" + FB2M_NAME_TABLE +  \n",
    "                  \" WHERE alias = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "def cached_aliases_to_mids(aliases):\n",
    "    mids = []\n",
    "    for alias in aliases:\n",
    "        mids.extend(cached_alias_to_mid(alias))\n",
    "    return mids\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_normalized_punctuation_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_preprocessed_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_preprocessed = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_normalized_punctuation_stem_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation_stem = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates - Upperbound\n",
    "\n",
    "Here we use the true alias, to compute the upperbound of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46cab518454453ba969940cca5dd28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21266), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answerable Precision: 1.000000 [21266 of 21266]\n",
      "Answerable Recall: 1.000000 [21266 of 21266]\n",
      "Expected Guessing Accuracy: 0.668482 [14497 of 21687]\n"
     ]
    }
   ],
   "source": [
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    candidate_aliases = [row['subject_name']]\n",
    "    candidates_mids.append(cached_aliases_to_mids(candidate_aliases))        \n",
    "\n",
    "evaluate_candidates(candidates_mids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates - Baseline\n",
    "\n",
    "Just lookup the top k predicted subject names in order until one is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e14a64adfe45f59941e35b9fa0dbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21266), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answerable Precision: 0.961781 [20409 of 21220]\n",
      "Answerable Recall: 0.997837 [21220 of 21266]\n",
      "Expected Guessing Accuracy: 0.634662 [13763 of 21687]\n"
     ]
    }
   ],
   "source": [
    "from lib.utils import format_pipe_table\n",
    "\n",
    "negative_sample = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for predicted in row['predicted_subject_names']:\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "        \n",
    "\n",
    "evaluate_candidates(candidates_mids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "\n",
    "For the first version, we will try to follow the strategy in `Normalized Reference Resolution#HYPOTHESIS - Subject Name not in Question.ipynb` to link more aliases to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedFunction",
     "evalue": "FEHLER:  Funktion similarity(unknown, unknown) existiert nicht\nLINE 1: SELECT similarity('hi', 'hey');\n               ^\nHINT:  Keine Funktion stimmt mit dem angegebenen Namen und den Argumenttypen überein. Sie müssen möglicherweise ausdrückliche Typumwandlungen hinzufügen.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedFunction\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-af1a6346999d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# TEST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpg_trgm_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hey'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-af1a6346999d>\u001b[0m in \u001b[0;36mpg_trgm_similarity\u001b[1;34m(text, other_text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Helper method to play with the metric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpg_trgm_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SELECT similarity(%s, %s);'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUndefinedFunction\u001b[0m: FEHLER:  Funktion similarity(unknown, unknown) existiert nicht\nLINE 1: SELECT similarity('hi', 'hey');\n               ^\nHINT:  Keine Funktion stimmt mit dem angegebenen Namen und den Argumenttypen überein. Sie müssen möglicherweise ausdrückliche Typumwandlungen hinzufügen.\n"
     ]
    }
   ],
   "source": [
    "# Helper method to play with the metric\n",
    "def pg_trgm_similarity(text, other_text):\n",
    "    cursor.execute('SELECT similarity(%s, %s);', (text, other_text))\n",
    "    similarity = cursor.fetchall()[0][0]\n",
    "    return similarity\n",
    "                   \n",
    "# TEST\n",
    "print(pg_trgm_similarity('hi', 'hey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9c9444c5094640b3134182a9ce2ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21266), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../..\\notebooks\\Simple QA Numbers\\HYPOTHESIS - Subject Name not in Question.ipynb\n",
      "\n"
     ]
    },
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "FEHLER:  aktuelle Transaktion wurde abgebrochen, Befehle werden bis zum Ende der Transaktion ignoriert\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-43d5879e5fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NORMALIZED_PUNCTUATION'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n\u001b[1;32m---> 17\u001b[1;33m                 text_normalize_punctuation(predicted['name']))\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_aliases\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-44d9082460e2>\u001b[0m in \u001b[0;36mcached_alias_normalized_punctuation_to_alias\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcached_alias_normalized_punctuation_to_alias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n\u001b[1;32m---> 18\u001b[1;33m                   \" WHERE alias_normalized_punctuation = %s\", (text,))\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInFailedSqlTransaction\u001b[0m: FEHLER:  aktuelle Transaktion wurde abgebrochen, Befehle werden bis zum Ende der Transaktion ignoriert\n"
     ]
    }
   ],
   "source": [
    "from lib.utils import format_pipe_table\n",
    "\n",
    "negative_samples = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        strategy = 'PREPROCESSED'\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            if row['subject'] not in candidates_mids[-1]:\n",
    "                considered_aliases = [predicted['name'] for j, predicted in \n",
    "                                          enumerate(row['predicted_subject_names']) if j <= i]\n",
    "                negative_samples.append({\n",
    "                    'Preprocessed Subject Name': text_preprocess(row['subject_name']),\n",
    "                    'Considered Aliases': considered_aliases,\n",
    "                    'Max Similarity': max([pg_trgm_similarity(row['subject_name'], a)\n",
    "                                           for a in considered_aliases]),\n",
    "                    'Predicted Alias': predicted['name'],\n",
    "                    'Strategy': strategy,\n",
    "                    'Question': row['question'],\n",
    "                })\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Negative Sample:')\n",
    "print(format_pipe_table(negative_samples[:50], columns=['Strategy', 'Max Similarity',\n",
    "                                                        'Preprocessed Subject Name',\n",
    "                                                        'Predicted Alias',\n",
    "                                                        'Considered Aliases', 'Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "\n",
    "Recall increased by 0.001784.\n",
    "Precision increased by 0.004104.\n",
    "\n",
    "\n",
    "##### Error Bucket:\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Handling possesives would fix 10 / 50 errors. Handling the `Similar` bucket would be difficult because it's typically because of extra words in the subject name not present in the question.\n",
    "\n",
    "**Buckets:**\n",
    "- Wrong Span (29 / 50): The wrong span in the question was selected\n",
    "- Suffix (12 / 50): The correct subject name was not linked due to a suffix.\n",
    "- Extra Article (3 / 50): The correct subject name was not linked due to an article.\n",
    "- Similar (7 / 50): The correct subject name was similar but not exact to the predicted subject name.\n",
    "- Other (1 / 50): Deeper reason that the correct subject name was not linked.\n",
    "\n",
    "| Index | Similarity | Bucket | Strategy | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 0.0 | Wrong Span | PREPROCESSED | short | documentary film | ['documentary film'] | Name a short documentary film released in 2011 |\n",
    "| 1 | 0.722222 | Suffix | PREPROCESSED | red cloud 's war | the red | ['the red clouds war', 'red clouds war', 'the red clouds', 'clouds war', 'red clouds', 'the red'] | what was involved in the red clouds war? |\n",
    "| 2 | 0.75 | Wrong Span | PREPROCESSED | corporation nation | nation | ['nation book', 'corporation nation book', 'the corporation nation book', 'nation'] | what subject is the corporation nation book about |\n",
    "| 3 | 0.8 | Suffix | PREPROCESSED | peter 's point plantation | peters | ['peters point plantation', 'peters point', 'point plantation', 'peters'] | What is peters point plantation's architectural style |\n",
    "| 4 | 0.0555556 | Wrong Span | PREPROCESSED | album | aaron carter | ['aaron carter'] | Name an album released by aaron carter |\n",
    "| 5 | 1.0 | Similar | PREPROCESSED | pillows & prayers : cherry red 1982–1983 | pillows & prayers : cherry red 1982 - 1983 | ['pillows & prayers : cherry red 1982 - 1983'] | What is the name of the track list for the release pillows & prayers: cherry red 1982-1983? |\n",
    "| 6 | 0.5 | Wrong Span | PREPROCESSED | commune of luxembourg | luxembourg | ['luxembourg'] | which country is the commune of luxembourg in |\n",
    "| 7 | 0.764706 | Extra Article | PREPROCESSED | the hits album 6 | 6 | ['hits album 6', '6'] | what song was included in the hits album 6 |\n",
    "| 8 | 0.588235 | Wrong Span | PREPROCESSED | between two women | two women | ['two women'] | what is about between two women |\n",
    "| 9 | 0.782609 | Suffix | PREPROCESSED | battle of hudson 's bay | bay | ['battle of hudsons bay', 'of hudsons bay', 'battle of hudsons', 'the battle of hudsons bay', 'hudsons bay', 'did the battle of hudsons bay', 'battle of', 'bay'] | where did the battle of hudsons bay take place |\n",
    "| 10 | 0.0 | Wrong Span | PREPROCESSED | tablet | hypertension | ['hypertension'] | what is a tablet used to treat hypertension  |\n",
    "| 11 | 0.0 | Wrong Span | PREPROCESSED | compilation album | frank zappa | ['frank zappa'] | what compilation album did frank zappa release? |\n",
    "| 12 | 0.0 | Wrong Span | PREPROCESSED | soundtrack | anthony marinelli | ['anthony marinelli'] | What's a soundtrack written by anthony marinelli |\n",
    "| 13 | 0.0 | Wrong Span | PREPROCESSED | album | george canyon | ['george canyon'] | name an album by George Canyon |\n",
    "| 14 | 0.0 | Wrong Span | PREPROCESSED | album | portal | ['portal'] | What's an album by the band portal |\n",
    "| 15 | 0.785714 | Suffix | PREPROCESSED | megan cheng | megan | ['megan chengs', 'megan'] | whats  megan chengs ethnicity |\n",
    "| 16 | 0.705882 | Wrong Span | PREPROCESSED | martial arts film | martial arts | ['martial arts'] | what is the name of the netflix martial arts film? |\n",
    "| 17 | 0.0222222 | Wrong Span | PREPROCESSED | creedence clearwater revival | compilation album | ['compilation album'] | What is a compilation album by creedence clearwater revival |\n",
    "| 18 | 0.227273 | Wrong Span | PREPROCESSED | topical medication | medicine | ['medicine'] | Name a topical medicine |\n",
    "| 19 | 0.636364 | Wrong Span | PREPROCESSED | master | the master | ['the master'] | what is one of the master's powers  |\n",
    "| 20 | 0.0 | Other | PREPROCESSED | t - town | kearny | ['kearny'] | What newspaper circulates in the town of kearny |\n",
    "| 21 | 0.571429 | Suffix | PREPROCESSED | drums | drum | ['drum'] | which musician plays the drum kit |\n",
    "| 22 | 0.84375 | Suffix | PREPROCESSED | dimillo 's floating restaurant | restaurant | ['dimillos floating restaurant', 'dimillos floating', 'floating restaurant', 'dimillos', 'is dimillos floating restaurant', 'dimillos floating restaurant in', 'restaurant'] | what state is dimillos floating restaurant in? |\n",
    "| 23 | 0.0 | Wrong Span | PREPROCESSED | ragtime | denmark | ['denmark'] | who is the ragtime artist born in denmark? |\n",
    "| 24 | 0.8 | Similar, Extra Article | PREPROCESSED | the regatta mystery | mystery | ['regatta mystery', 'mystery'] | what theme is in the piece regatta mystery |\n",
    "| 25 | 0.0 | Wrong Span | PREPROCESSED | album | jack | ['jack dejohnrette', 'jack'] | What is the name of Jack DeJohnrette's album? |\n",
    "| 26 | 0.0 | Wrong Span | PREPROCESSED | bollywood | tamil | ['tamil'] | what bollywood Tamil film was released in 2004  |\n",
    "| 27 | 0.0 | Wrong Span | PREPROCESSED | animated cartoon | ducks | ['ducks'] | what animated cartoon was about ducks? |\n",
    "| 28 | 0.0 | Wrong Span | PREPROCESSED | photography | visual art | ['visual art'] | which artist uses photography as their preferred visual art form |\n",
    "| 29 | 0.761905 | Suffix | PREPROCESSED | this pud 's for you | for you | ['this puds for you comes', 'this puds for you', 'this puds for you comes from', 'puds for you comes', 'puds for you', 'this puds for', 'this puds', 'episode this puds for you comes', 'for you comes', 'puds for you comes from', 'episode this puds for you', 'for you'] | what is the series where the episode this puds for you comes from |\n",
    "| 30 | 0.826087 | Suffix | NORMALIZED | chet 's speech , part ii | , part ii | ['chets speech , part ii', 'speech , part ii', 'chets speech , part', 'chets speech ,', 'chets speech', ', part ii'] | who sings chets speech, part ii |\n",
    "| 31 | 0.764706 | Wrong Span | PREPROCESSED | large family car | family | ['large family', 'family'] | What car model is an example of a large family car? |\n",
    "| 32 | 0.761905 | Suffix | PREPROCESSED | men 's pommel horse | pommel horse | ['mens pommel horse', 'mens pommel', 'pommel horse'] | What olympic games featured mens pommel horse |\n",
    "| 33 | 0.0526316 | Wrong Span | NORMALIZED | soundtrack | s.cry.ed | ['s.cry.ed'] | What's the soundtrack for s.cry.ed |\n",
    "| 34 | 0.35 | Wrong Span | PREPROCESSED | sahara ( instrumental ) | sahara | ['sahara'] | who composed sahara (instrumental)? |\n",
    "| 35 | 0.0625 | Wrong Span | PREPROCESSED | compilation | cema | ['albumby cema', 'cema'] | what album is released as a compilation albumby CEMA |\n",
    "| 36 | 0.583333 | Wrong Span | PREPROCESSED | arabic name | arabic | ['arabic'] | What is a book that is about arabic name |\n",
    "| 37 | 0.73913 | Similar | PREPROCESSED | multiplayer video game | game | ['multiplayer game', 'game'] | What's a text based multiplayer game |\n",
    "| 38 | 0.75 | Extra Article | PREPROCESSED | the crystal city | crystal city | ['crystal city'] | what genre is crystal city |\n",
    "| 39 | 0.84 | Suffix | PREPROCESSED | men 's badminton , singles | singles | ['mens badminton , singles', 'badminton , singles', 'mens badminton', 'mens badminton ,', 'singles'] | what olympic games was mens badminton, singles apart of |\n",
    "| 40 | 0.0 | Wrong Span | PREPROCESSED | mercedes lackey | fantasy | ['fantasy'] | which fantasy series were written by mercedes lackey? |\n",
    "| 41 | 0.666667 | Similar | PREPROCESSED | brian o'shea | brian oshea | ['brian oshea'] | brian oshea performs what type of martial art |\n",
    "| 42 | 0.857143 | Similar | PREPROCESSED | u.s . office of war information | war | ['office of war information', 'office of war information help', 'the office of war information', 'war information', 'the office of war information help', 'of war information', 'office of war', 'war information help', 'of war information help', 'the office of war', 'office of war information help produce', 'war'] | which film did the office of war information help produce  |\n",
    "| 43 | 0.0 | Wrong Span | PREPROCESSED | album | sham 69 | ['sham 69'] | which album is released by Sham 69 |\n",
    "| 44 | 0.777778 | Wrong Span | NORMALIZED | lowthian bell | , 1st baronet | ['sir lowthian bell , 1st baronet', 'lowthian bell , 1st baronet', 'sir lowthian bell , 1st', 'sir lowthian bell ,', 'sir lowthian bell', 'bell , 1st baronet', 'sir lowthian', ', 1st baronet'] | what organization was founded by sir lowthian bell, 1st baronet |\n",
    "| 45 | 0.862069 | Suffix | PREPROCESSED | st . peter 's episcopal church | st . peters | ['st . peters episcopal church', 'st . peters episcopal', '. peters episcopal church', 'peters episcopal church', 'st . peters'] | what state and city is st. peters episcopal church located in? |\n",
    "| 46 | 0.851852 | Suffix | PREPROCESSED | richard scarry 's busytown | busytown | ['richard scarrys busytown', 'scarrys busytown', 'richard scarrys', 'busytown'] | what is a gameplay mode featured on richard scarrys busytown |\n",
    "| 47 | 0.0 | Wrong Span | PREPROCESSED | album | soil | ['soil'] | What's an album by soil |\n",
    "| 48 | 0.714286 | Similar | PREPROCESSED | texas a&m university school of law | texas wesleyan university | ['texas wesleyan university school of law', 'wesleyan university school of law', 'texas wesleyan university school of', 'texas wesleyan university school', 'university school of law', 'is texas wesleyan university school of law', 'texas wesleyan university'] | Where is texas wesleyan university school of law located? |\n",
    "| 49 | 0.535714 | Wrong Span | PREPROCESSED | public service announcement | public service | ['public service'] | What is the name of a public service announcement? |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2\n",
    "\n",
    "In version 1, the error bucketing revealed a failure to handle suffix's; therefore, we proceed to handle them in version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import format_pipe_table\n",
    "\n",
    "negative_samples = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        strategy = 'PREPROCESSED'\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION_STEM'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            if row['subject'] not in candidates_mids[-1]:\n",
    "                considered_aliases = [predicted['name'] for j, predicted in \n",
    "                                          enumerate(row['predicted_subject_names']) if j <= i]\n",
    "                negative_samples.append({\n",
    "                    'Preprocessed Subject Name': text_preprocess(row['subject_name']),\n",
    "                    'Considered Aliases': considered_aliases,\n",
    "                    'Max Similarity': max([pg_trgm_similarity(text_normalize_punctuation_stem(row['subject_name']),\n",
    "                                                              text_normalize_punctuation_stem(a))\n",
    "                                           for a in considered_aliases]),\n",
    "                    'Predicted Alias': predicted['name'],\n",
    "                    'Strategy': strategy,\n",
    "                    'Question': row['question'],\n",
    "                })\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Negative Sample:')\n",
    "# To not overfit on the first 50 samples\n",
    "print(format_pipe_table(negative_samples[50:100], columns=['Strategy', 'Max Similarity',\n",
    "                                                        'Preprocessed Subject Name',\n",
    "                                                        'Predicted Alias',\n",
    "                                                        'Considered Aliases', 'Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "\n",
    "Recall increased by 0.000376.\n",
    "\n",
    "Precision increased by 0.004896.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "We managed to increase our expected accuracy by a 1% with these simple normalization measures. We believe this will lead to a 1% + gain downstream.\n",
    "\n",
    "##### Error Bucket:\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "For the most part, the wrong span is selected; therefore, the alias is difficult to link with the current implementation of the algorithm.\n",
    "\n",
    "We can handle the similar case some what. The similarity between the correct alias and the subject name tends to be fairly high; therefore, we can try adding a step to look for any aliases that are similar with a score of 0.85+. We expect this to handle  3 / 50 errors.\n",
    "\n",
    "| Max Similarity | Bucket |\n",
    "| --- | --- |\n",
    "| 0.888889 | Similar |\n",
    "| 0.714286 | Similar |\n",
    "| 0.875 | Similar |\n",
    "| 1.0 | Similar |\n",
    "| 0.62069 | Similar |\n",
    "| 1.0 | Similar |\n",
    "| 0.88 | Similar |\n",
    "\n",
    "**Buckets:**\n",
    "- Wrong Span (42 / 50): The wrong span in the question was selected.\n",
    "- Similar (8 / 50): The correct subject name was similar but not exact to the predicted subject name.\n",
    "- Extra Article (2 / 50): The correct subject name was not linked due to an article.\n",
    "\n",
    "| Index | Max Similarity  | Bucket | Strategy | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 0.705882 | Wrong Span | PREPROCESSED | pim fortuyn list | pim fortuyn | ['pim fortuyn'] | what ideology does the pim fortuyn list follow |\n",
    "| 1 | 0.0 | Wrong Span | NORMALIZED_PUNCTUATION_STEM | drug | cleaning | ['cleaning hands', 'for cleaning hands', 'cleaning'] | what drug is used for cleaning hands  |\n",
    "| 2 | 0.444444 | Wrong Span | PREPROCESSED | leather subculture | leather | ['leather'] | what's one event that celebrates the leather subculture |\n",
    "| 3 | 0.333333 | Wrong Span | PREPROCESSED | illinois river | rogue river | ['lower rogue river', 'rogue river'] | in which community does the illinois river confluence with the Lower Rogue river |\n",
    "| 4 | 0.764706 | Wrong Span, Extra Article | PREPROCESSED | need for speed | the need for speed | ['the need for speed'] | what type of film is the need for speed |\n",
    "| 5 | 0.0 | Wrong Span | PREPROCESSED | asteroid | geologist | ['geologist'] | which asteroid was names after an italian geologist? |\n",
    "| 6 | 0.0 | Wrong Span | PREPROCESSED | album | chico debarge | ['chico debarge'] | what is an album by Chico DeBarge? |\n",
    "| 7 | 1.0 | Punctuation | PREPROCESSED | unter null . | unter null | ['unter null'] | what type of book binding is unter null. |\n",
    "| 8 | 0.5 | Wrong Span | NORMALIZED_PUNCTUATION | k - pop | - pop music | ['k - pop music', '- pop music'] | Who is an artist of k-pop music? |\n",
    "| 9 | 0.0454545 | Wrong Span | PREPROCESSED | novel | vladimir nabokov | ['vladimir nabokov'] | Name a novel by Vladimir Nabokov |\n",
    "| 10 | 0.888889 | Similar | PREPROCESSED | millard s drexler | millard | ['millard drexler', 'drexler', 'millard'] | What organization did millard drexler found |\n",
    "| 11 | 0.0 | Wrong Span | PREPROCESSED | musical | israel | ['israel'] | which musical films were broadcasted in israel? |\n",
    "| 12 | 0.0 | Wrong Span | PREPROCESSED | live album | 3oh!3 | ['3oh!3'] | what is the name of the live album by 3oh!3 |\n",
    "| 13 | 0.5 | Wrong Span | PREPROCESSED | epic film | epic | ['epic'] | Name a 1936 epic film  |\n",
    "| 14 | 0.533333 | Wrong Span | PREPROCESSED | roy rogers restaurants | roy rogers | ['roy rogers'] | roy rogers restaurants in which industry? |\n",
    "| 15 | 0.0 | Wrong Span | PREPROCESSED | caucasian | babylon 5 | ['babylon 5'] | who is of caucasian race in babylon 5 |\n",
    "| 16 | 0.6 | Wrong Span | PREPROCESSED | painting | visual art | ['visual art painting', 'art painting', 'visual art'] | what is a visual art painting |\n",
    "| 17 | 0.0 | Wrong Span | PREPROCESSED | album | funk | ['koul funk', 'funk'] | what album was release by Koul Funk |\n",
    "| 18 | 0.0555556 | Wrong Span | PREPROCESSED | science | brian swimme | ['brian swimme'] | brian swimme wrote what book that dealt with  science |\n",
    "| 19 | 0.0 | Wrong Span | PREPROCESSED | animation | raoul servais | ['raoul servais'] | Which animation did Raoul Servais direct |\n",
    "| 20 | 0.333333 | Wrong Span | PREPROCESSED | working title films | films | ['films'] | what is the film from the production company working title films |\n",
    "| 21 | 0.0344828 | Wrong Span | PREPROCESSED | jerry bruckheimer | biographical film | ['biographical film'] | jerry bruckheimer was the producer of this biographical film.  |\n",
    "| 22 | 0.714286 | Similar | PREPROCESSED | godbout v longueuil ( city of ) | longueuil | ['godbout v. longueuil', 'godbout v.', 'v. longueuil', 'the godbout v. longueuil', 'godbout v. longueuil case', 'godbout', 'longueuil'] | what court handled the godbout v. longueuil case? |\n",
    "| 23 | 0.875 | Similar | PREPROCESSED | raymond a. meier | raymond | ['raymond meier', 'meier', 'was raymond meier', 'raymond'] | Which city was raymond meier born in |\n",
    "| 24 | 0.714286 | Wrong Span | PREPROCESSED | afterglow | the afterglow | ['the afterglow fil', 'the afterglow fil ,', 'afterglow fil', 'afterglow fil ,', 'the afterglow'] | who did the music for the afterglow fil, |\n",
    "| 25 | 0.0 | Wrong Span | PREPROCESSED | album | leo sayer | ['leo sayer'] | what is an album by leo sayer? |\n",
    "| 26 | 1.0 | Similar | PREPROCESSED | drums | drum | ['drum'] | who played the drum  in the Los Angeles rock quintet Rooney |\n",
    "| 27 | 0.625 | Wrong Span | PREPROCESSED | latin pop | pop music | ['latin pop music', 'pop music'] | who is an artist that creates latin pop music |\n",
    "| 28 | 0.0909091 | Wrong Span | PREPROCESSED | death eaters | harry potter | ['harry potter'] | which is the name of a death eater in harry potter? |\n",
    "| 29 | 0.62069 | Similar | NORMALIZED_PUNCTUATION | single - player video game | single - | ['single - player mode', 'single - player mode game', 'single - player', 'single -'] | what is a single-player mode game? |\n",
    "| 30 | 0.461538 | Wrong Span | PREPROCESSED | album | compilation album | ['compilation album'] | What is a compilation album from 2006  |\n",
    "| 31 | 0.047619 | Wrong Span | PREPROCESSED | studio album | arcangel | ['arcangel'] | What was a studio album recording for Arcángel |\n",
    "| 32 | 0.6 | Wrong Span | PREPROCESSED | saint | the saint | ['the saint novel', 'saint novel', 'the saint'] | What type of book is the saint novel? |\n",
    "| 33 | 0.5 | Wrong Span | PREPROCESSED | avila place | avila | ['avila'] | what western state does contain avila place |\n",
    "| 34 | 0.0357143 | Wrong Span | PREPROCESSED | north carolina | surrey | ['surrey county', 'surrey'] | What is a city in Surrey County, north carolina? |\n",
    "| 35 | 0.0 | Wrong Span | PREPROCESSED | album | johannes brahms | ['johannes brahms'] | What is an album written by Johannes Brahms |\n",
    "| 36 | 0.777778 | Wrong Span | PREPROCESSED | first battle of james island | james island | ['battle of james island', 'battle of james', 'of james island', 'james island'] | Name a soldier involved in the battle of james island. |\n",
    "| 37 | 0.642857 | Wrong Span | PREPROCESSED | plymouth | plymouth rock | ['plymouth rock'] | is there another attraction in plymouth other than plymouth rock |\n",
    "| 38 | 0.705882 | Wrong Span | PREPROCESSED | altered beast | beast | ['beast game', 'altered beast game', 'beast'] | who is the creator of the altered beast game |\n",
    "| 39 | 0.45 | Wrong Span | PREPROCESSED | the barefoot artist | barefoot | ['barefoot'] | which film created the barefoot artist  |\n",
    "| 40 | 0.352941 | Wrong Span | PREPROCESSED | pornographic actor | actor | ['actor'] | who is a pornographic actor |\n",
    "| 41 | 0.0 | Wrong Span | PREPROCESSED | album | century media | ['century media'] | which albums were released by the century media label? |\n",
    "| 42 | 0.5 | Wrong Span | PREPROCESSED | 8833 acer | acer | ['acer'] | what is a 8833 acer |\n",
    "| 43 | 1.0 | Similar | PREPROCESSED | cruisin ' | cruisin | ['cruisin'] | What release is cruisin on? |\n",
    "| 44 | 0.88 | Similar | NORMALIZED_PUNCTUATION_STEM | the wonderful wizard of ha 's | the wonderful | ['the wonderful wizard of', 'the wonderful wizard of has', 'the wonderful wizard', 'wonderful wizard of', 'wonderful wizard of has', 'wonderful wizard', 'wizard of', 'the wonderful'] | What film series is the wonderful wizard of has from? |\n",
    "| 45 | 0.842105 | Wrong Span, Article | PREPROCESSED | tower of london | the tower of london | ['the tower of london'] | who recorded the tower of london |\n",
    "| 46 | 0.7 | Wrong Span | PREPROCESSED | outside in | outside | ['outside'] | Which genre is outside in associated with |\n",
    "| 47 | 0.4 | Wrong Span | PREPROCESSED | rca | rca records | ['rca records'] | Who is an artist  signed by rca records? |\n",
    "| 48 | 0.0 | Wrong Span | PREPROCESSED | action game | sega | ['sega'] | What's an action game made by sega |\n",
    "| 49 | 0.647059 | Wrong Span | PREPROCESSED | film adaptation | novel | ['novel film adaptation', 'novel'] | What's an example of a novel film adaptation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3\n",
    "\n",
    "In version 2, the error bucketing revealed a failure to handling similar aliases's; therefore, we proceed to handle them in version 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=65536)\n",
    "def cached_similar_alias_normalized_punctuation_stem_to_alias(text, limit):\n",
    "    cursor.execute(\"\"\"SELECT set_limit(\"\"\" + str(limit) + \"\"\");\n",
    "                    SELECT DISTINCT alias FROM fb_two_subject_name \n",
    "                    WHERE alias_normalized_punctuation_stem %% %s\"\"\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import format_pipe_table\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    limit = 0.8\n",
    "    while limit > 0:\n",
    "        is_break = False\n",
    "        for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "            candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "\n",
    "            # Punctuation Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                    text_normalize_punctuation(predicted['name']))\n",
    "\n",
    "            # Suffix Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']))\n",
    "\n",
    "            # Other Similar Aliases\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']), limit)\n",
    "\n",
    "            if len(candidate_aliases) > 0:\n",
    "                candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "                n_aliases += len(candidate_aliases)\n",
    "                is_break = True\n",
    "                break\n",
    "        if is_break:\n",
    "            break\n",
    "        limit -= 0.1\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average Number of Aliases:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "    \n",
    "Version 3\n",
    "- Precision: 0.974171 [10372 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
    "- Average Number Of Aliases: 1.0379413974455296\n",
    "\n",
    "Recall stayed the same.\n",
    "Precision increased by 0.000751.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "The increase here is small; therefore, it may not be worth it to include this last step in the pipeline. Without this last step, there is little room to grow otherwise with SQL queries. \"Average Number of Aliases\" does indicate that there is some room to grow in filtering out aliases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4\n",
    "\n",
    "In Version 4, we investigate alias filtering via mean candidate distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider picking the alias that on average is closest to the candidate. The average summing over all\n",
    "# aliases for that MID.\n",
    "\n",
    "from Levenshtein import distance\n",
    "from lib.utils import format_pipe_table\n",
    "import statistics\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']), 0.8)\n",
    "\n",
    "        if len(candidate_aliases) > 0:\n",
    "            # Filter by smallest edit distance to originally predicted name\n",
    "            score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "            best_score = min([score(a) for a in candidate_aliases])\n",
    "            candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "            \n",
    "            # Copute the number of aliases\n",
    "            n_aliases += len(candidate_aliases)\n",
    "            mids = cached_aliases_to_mids(candidate_aliases)\n",
    "            scores = []\n",
    "            # IF there exists more aliases for a mid, we average\n",
    "            for mid in mids:\n",
    "                cursor.execute('SELECT alias FROM fb_two_name WHERE mid = %s', (mid,))\n",
    "                score = statistics.mean([distance(r[0], predicted['name']) for r in cursor.fetchall()])\n",
    "                scores.append(score)\n",
    "            min_score = min(scores)\n",
    "            mids = [mid for i, mid in enumerate(mids) if scores[i] == min_score]\n",
    "            candidates_mids.append(mids)\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average number of alaises:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 5\n",
    "\n",
    "In Version 5, we investigate alias filtering via edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider picking the alias that on average is closest to the candidate. The average summing over all\n",
    "# aliases for that MID.\n",
    "# TODO: Consider picking the alias with the largest amount of aggragate object mids\n",
    "\n",
    "from Levenshtein import distance\n",
    "from lib.utils import format_pipe_table\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']), 0.8)\n",
    "\n",
    "        if len(candidate_aliases) > 0:\n",
    "            # Filter by smallest edit distance to originally predicted name\n",
    "            score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "            best_score = min([score(a) for a in candidate_aliases])\n",
    "            candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "            \n",
    "            # Copute the number of aliases\n",
    "            n_aliases += len(candidate_aliases)\n",
    "            \n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average number of alaises:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "    \n",
    "Version 3\n",
    "- Precision: 0.974171 [10372 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
    "- Average Number Of Aliases: 1.0379413974455296\n",
    "\n",
    "Version 5\n",
    "- Precision: 0.972387 [10353 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.676108 [7199 of 10648]\n",
    "- Average number of alaises: 1.000939143501127\n",
    "\n",
    "Recall stayed the same.\n",
    "Precision decreased by 0.001784.\n",
    "The expected accuracy went up 0.00655 by close to half a percent.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "With a small decrease in percision, we were able to reduce the number of aliases to choose from. Resulting in a 0.65% increase in our expected accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin\n",
    "\n",
    "Here we use our algorithm to generate candidates and save the results of Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "from numpy import nan\n",
    "\n",
    "def generate_candidates(cursor, row):\n",
    "    limit = 0.85\n",
    "    while limit > 0:\n",
    "        for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "            candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "            \n",
    "            # Punctuation Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                    text_normalize_punctuation(predicted['name']))\n",
    "\n",
    "            # Suffix Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']))\n",
    "\n",
    "            # Other Similar Aliases\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']), limit)\n",
    "\n",
    "            if len(candidate_aliases) > 0:\n",
    "                # Filter by smallest edit distance to originally predicted name\n",
    "                # TODO: Look into filtering after the relation filter\n",
    "                score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "                best_score = min([score(a) for a in candidate_aliases])\n",
    "                candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "                mids = cached_aliases_to_mids(candidate_aliases)\n",
    "                row['candidate_mids'] = mids\n",
    "                row['predicted_start_index'] = predicted['start_index']\n",
    "                row['predicted_end_index'] = predicted['end_index']\n",
    "                row['predicted_subject_name'] = predicted['name']\n",
    "                return row\n",
    "        limit -= 0.1\n",
    "        \n",
    "    row['candidate_mids'] = []\n",
    "    row['predicted_start_index'] = nan\n",
    "    row['predicted_end_index'] = nan\n",
    "    row['predicted_subject_name'] = nan\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "df = df.progress_apply(partial(generate_candidates, cursor), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity\n",
    "\n",
    "Check if `generate_candidates` works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy just to check the implementation of `generate_candidates`.\n",
    "correct = 0\n",
    "expected_correct = 0\n",
    "subject_name_correct = 0\n",
    "for index, row in tqdm_notebook(df.iterrows(), total=df.shape[0]):\n",
    "    if row['subject'] in row['candidate_mids']:\n",
    "        correct += 1\n",
    "        expected_correct += 1 / len(row['candidate_mids'])\n",
    "    \n",
    "    if (isinstance(row['subject_name'], str) and\n",
    "        text_preprocess(row['subject_name']) == row['predicted_subject_name']):\n",
    "        subject_name_correct += 1\n",
    "        \n",
    "print('Candidate Accuracy: %f [%d of %d]' % (correct / df.shape[0], correct, df.shape[0]))\n",
    "print('Expected Accuracy: %f [%d of %d]' % (expected_correct / df.shape[0], expected_correct, df.shape[0]))\n",
    "# TODO: Look at subject names that are incorrect but the subject is correct\n",
    "# Because that's weird.\n",
    "print('Subject Name Accuracy: %f [%d of %d]' %\n",
    "      (subject_name_correct / df.shape[0],subject_name_correct, df.shape[0]))\n",
    "\n",
    "# Candidate Accuracy: 0.951169 [20628 of 21687]\n",
    "# Expected Accuracy: 0.650218 [14101 of 21687]\n",
    "# Subject Name Accuracy: 0.928252 [20131 of 21687]\n",
    "\n",
    "# Candidate Accuracy: 0.953521 [20679 of 21687]\n",
    "# Expected Accuracy: 0.652421 [14149 of 21687]\n",
    "# Subject Name Accuracy: 0.922119 [19998 of 21687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Pipeline\n",
    "\n",
    "Write step 2 results to use them in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('step_2_generate_candidates.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = connection.cursor()\n",
    "curs.execute(\"ROLLBACK\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
