{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Generate Candidates\n",
    "\n",
    "Our goal during this step is to generate candidate mids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.connect import get_connection \n",
    "from lib.data import FB2M_NAME_TABLE\n",
    "\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b4ee9565545059ff8d1462ad00336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_index</th>\n",
       "      <th>object</th>\n",
       "      <th>predicted_question_tokens</th>\n",
       "      <th>predicted_subject_names</th>\n",
       "      <th>question</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>relation</th>\n",
       "      <th>start_index</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_name</th>\n",
       "      <th>subject_name_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17188</th>\n",
       "      <td>7.0</td>\n",
       "      <td>075s73</td>\n",
       "      <td>[which, town, is, in, new, york, city]</td>\n",
       "      <td>[{'name': 'new york city', 'score': 64.1403045...</td>\n",
       "      <td>which town is in new york city</td>\n",
       "      <td>[which, town, is, in, new, york, city]</td>\n",
       "      <td>location/place_with_neighborhoods/neighborhoods</td>\n",
       "      <td>4.0</td>\n",
       "      <td>02_286</td>\n",
       "      <td>new york city</td>\n",
       "      <td>(new, york, city)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>4.0</td>\n",
       "      <td>017drs</td>\n",
       "      <td>[does, pee, wee, reese, play, shortstop, or, p...</td>\n",
       "      <td>[{'name': 'pee wee reese', 'score': 91.0824661...</td>\n",
       "      <td>does pee wee reese play shortstop or power for...</td>\n",
       "      <td>[does, pee, wee, reese, play, shortstop, or, p...</td>\n",
       "      <td>baseball/baseball_player/position_s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01bg1k</td>\n",
       "      <td>pee wee reese</td>\n",
       "      <td>(pee, wee, reese)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21187</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0vp3fq</td>\n",
       "      <td>[what, is, a, track, by, lutricia, mcneal, ?]</td>\n",
       "      <td>[{'name': 'lutricia mcneal', 'score': 79.92938...</td>\n",
       "      <td>What is a track by lutricia mcneal?</td>\n",
       "      <td>[what, is, a, track, by, lutricia, mcneal, ?]</td>\n",
       "      <td>music/artist/track</td>\n",
       "      <td>5.0</td>\n",
       "      <td>06tw28</td>\n",
       "      <td>lutricia  mcneal</td>\n",
       "      <td>(lutricia, mcneal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18730</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0wzyx1</td>\n",
       "      <td>[name, a, recording, by, nelson, mandela]</td>\n",
       "      <td>[{'name': 'nelson mandela', 'score': 57.336029...</td>\n",
       "      <td>Name a recording by nelson mandela</td>\n",
       "      <td>[name, a, recording, by, nelson, mandela]</td>\n",
       "      <td>music/release_track/recording</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0slws_1</td>\n",
       "      <td>nelson mandela</td>\n",
       "      <td>(nelson, mandela)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0crryw4</td>\n",
       "      <td>[name, a, film, in, the, netflix, genre, celti...</td>\n",
       "      <td>[{'name': 'celtic music', 'score': 106.7388153...</td>\n",
       "      <td>Name a film in the netflix genre celtic music.</td>\n",
       "      <td>[name, a, film, in, the, netflix, genre, celti...</td>\n",
       "      <td>media_common/netflix_genre/titles</td>\n",
       "      <td>7.0</td>\n",
       "      <td>01m1y</td>\n",
       "      <td>celtic music</td>\n",
       "      <td>(celtic, music)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       end_index   object                          predicted_question_tokens  \\\n",
       "17188        7.0   075s73             [which, town, is, in, new, york, city]   \n",
       "4793         4.0   017drs  [does, pee, wee, reese, play, shortstop, or, p...   \n",
       "21187        7.0   0vp3fq      [what, is, a, track, by, lutricia, mcneal, ?]   \n",
       "18730        6.0   0wzyx1          [name, a, recording, by, nelson, mandela]   \n",
       "10014        9.0  0crryw4  [name, a, film, in, the, netflix, genre, celti...   \n",
       "\n",
       "                                 predicted_subject_names  \\\n",
       "17188  [{'name': 'new york city', 'score': 64.1403045...   \n",
       "4793   [{'name': 'pee wee reese', 'score': 91.0824661...   \n",
       "21187  [{'name': 'lutricia mcneal', 'score': 79.92938...   \n",
       "18730  [{'name': 'nelson mandela', 'score': 57.336029...   \n",
       "10014  [{'name': 'celtic music', 'score': 106.7388153...   \n",
       "\n",
       "                                                question  \\\n",
       "17188                     which town is in new york city   \n",
       "4793   does pee wee reese play shortstop or power for...   \n",
       "21187                What is a track by lutricia mcneal?   \n",
       "18730                 Name a recording by nelson mandela   \n",
       "10014     Name a film in the netflix genre celtic music.   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "17188             [which, town, is, in, new, york, city]   \n",
       "4793   [does, pee, wee, reese, play, shortstop, or, p...   \n",
       "21187      [what, is, a, track, by, lutricia, mcneal, ?]   \n",
       "18730          [name, a, recording, by, nelson, mandela]   \n",
       "10014  [name, a, film, in, the, netflix, genre, celti...   \n",
       "\n",
       "                                              relation  start_index  subject  \\\n",
       "17188  location/place_with_neighborhoods/neighborhoods          4.0   02_286   \n",
       "4793               baseball/baseball_player/position_s          1.0   01bg1k   \n",
       "21187                               music/artist/track          5.0   06tw28   \n",
       "18730                    music/release_track/recording          4.0  0slws_1   \n",
       "10014                media_common/netflix_genre/titles          7.0    01m1y   \n",
       "\n",
       "           subject_name subject_name_tokens  \n",
       "17188     new york city   (new, york, city)  \n",
       "4793      pee wee reese   (pee, wee, reese)  \n",
       "21187  lutricia  mcneal  (lutricia, mcneal)  \n",
       "18730    nelson mandela   (nelson, mandela)  \n",
       "10014      celtic music     (celtic, music)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "df = pd.read_pickle('step_1_predict_subject_name.pkl')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define text preprocessing the same as the training data and step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../notebooks/Simple QA Models/Subject Recognition Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import lib.import_notebook\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "PREPROCESS = importlib.import_module(\n",
    "                \"notebooksSimple QA Models.Subject Recognition Data\").preprocess\n",
    "TOKENIZE = importlib.import_module(\n",
    "                \"notebooksSimple QA Models.Subject Recognition Data\").spacy_tokenize\n",
    "\n",
    "def text_preprocess(s):\n",
    "    # Define `text_preprocess` the way the input text was preprocessed before step 1\n",
    "    s = PREPROCESS(s)\n",
    "    s = TOKENIZE(s)\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "def text_normalize_punctuation(s):\n",
    "    s = text_preprocess(s)\n",
    "    # In `Normalized Reference Resolution#HYPOTHESIS - Subject Name not in Question.ipynb` we found that\n",
    "    # aliases and questions match up more if punctuation is removed.\n",
    "    # Remove punctuation\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    # Removing characters can create gaps of multiple spaces\n",
    "    # Substitue multiple spaces with one\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def text_normalize_punctuation_stem(s):\n",
    "    s = text_preprocess(s)\n",
    "    \n",
    "    # Remove Possessives\n",
    "    tokens = s.split()\n",
    "    possessives = [\"'s\"]\n",
    "    tokens = [t for t in tokens if t not in possessives]\n",
    "    # Stem\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    s = ' '.join(tokens)\n",
    "    \n",
    "    s = text_normalize_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Subject Aliases\n",
    "\n",
    "Create an index of subject aliases that are preprocessed similar to the predicted subect name. Allowing for a database lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_normalized_punctuation varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_normalized_punctuation_stem varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_preprocessed varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import psycopg2\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "def update_chunk(rows):\n",
    "    query = ('UPDATE ' + FB2M_NAME_TABLE + ' SET alias_preprocessed = %s, ' +\n",
    "             'alias_normalized_punctuation = %s, alias_normalized_punctuation_stem = %s ' +\n",
    "             'WHERE mid = %s and alias = %s')\n",
    "    psycopg2.extras.execute_batch(cursor, query, rows)\n",
    "\n",
    "cursor.execute('SELECT mid, alias FROM ' + FB2M_NAME_TABLE)\n",
    "rows = []\n",
    "for mid, alias in tqdm_notebook(cursor.fetchall()):\n",
    "    alias_preprocessed = text_preprocess(alias)\n",
    "    alias_normalized_punctuation = text_normalize_punctuation(alias)\n",
    "    alias_normalized_punctuation_stem = text_normalize_punctuation_stem(alias)\n",
    "    rows.append(tuple([alias_preprocessed, alias_normalized_punctuation, \n",
    "                       alias_normalized_punctuation_stem, mid, alias]))\n",
    "    \n",
    "    # Insert Chunk\n",
    "    if len(rows) > chunk_size:\n",
    "        update_chunk(rows)\n",
    "        rows = []\n",
    "        \n",
    "update_chunk(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_preprocessed ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_preprocessed);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_normalized_punctuation);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_normalized_punctuation_stem);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem_trgm ON ' + \n",
    "               FB2M_NAME_TABLE + ' USING gist(alias_normalized_punctuation_stem gist_trgm_ops);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If subject name is null, then the question does not refer to the true alias. The example is then unanswerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answerable = df[df.subject_name.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics used to evaluate different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_candidates(candidates_mids):\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "    expected_accuracy = 0\n",
    "    n_answerable_examples = df_answerable.shape[0]\n",
    "    n_examples = df.shape[0]\n",
    "\n",
    "    for i, (_, row) in enumerate(df_answerable.iterrows()):\n",
    "        mids = candidates_mids[i]\n",
    "        if len(mids) == 0:\n",
    "            skipped += 1\n",
    "        elif row['subject'] in mids:\n",
    "            correct += 1\n",
    "            expected_accuracy += 1 / len(mids)\n",
    "        \n",
    "    print('Answerable Precision: %f [%d of %d]' %\n",
    "              (correct / (n_answerable_examples - skipped), correct,\n",
    "               (n_answerable_examples - skipped)))\n",
    "    print('Answerable Recall: %f [%d of %d]' %\n",
    "              ((n_answerable_examples - skipped) / n_answerable_examples,\n",
    "               (n_answerable_examples - skipped), n_answerable_examples))\n",
    "    print('Expected Guessing Accuracy: %f [%d of %d]' % \n",
    "              (expected_accuracy / n_examples, expected_accuracy, n_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic helper functions to run experiments quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_to_mid(text):\n",
    "    cursor.execute(\"SELECT mid FROM \" + FB2M_NAME_TABLE +  \n",
    "                  \" WHERE alias = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "def cached_aliases_to_mids(aliases):\n",
    "    mids = []\n",
    "    for alias in aliases:\n",
    "        mids.extend(cached_alias_to_mid(alias))\n",
    "    return mids\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_normalized_punctuation_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_preprocessed_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_preprocessed = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_normalized_punctuation_stem_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation_stem = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates - Upperbound\n",
    "\n",
    "Here we use the true alias, to compute the upperbound of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07561f41cf544496910adc322119e3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answerable Precision: 1.000000 [21188 of 21188]\n",
      "Answerable Recall: 1.000000 [21188 of 21188]\n",
      "Expected Guessing Accuracy: 0.667733 [14427 of 21607]\n"
     ]
    }
   ],
   "source": [
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    candidate_aliases = [row['subject_name']]\n",
    "    candidates_mids.append(cached_aliases_to_mids(candidate_aliases))        \n",
    "\n",
    "evaluate_candidates(candidates_mids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates - Baseline\n",
    "\n",
    "Just lookup the top k predicted subject names in order until one is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d0b7e630ef48ad85e1b7628b2c8854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answerable Precision: 0.963693 [20332 of 21098]\n",
      "Answerable Recall: 0.995752 [21098 of 21188]\n",
      "Expected Guessing Accuracy: 0.634341 [13706 of 21607]\n"
     ]
    }
   ],
   "source": [
    "from lib.table import format_pipe_table\n",
    "\n",
    "negative_sample = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for predicted in row['predicted_subject_names']:\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "        \n",
    "\n",
    "evaluate_candidates(candidates_mids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "\n",
    "For the first version, we will try to follow the strategy in `Normalized Reference Resolution#HYPOTHESIS - Subject Name not in Question.ipynb` to link more aliases to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166667\n"
     ]
    }
   ],
   "source": [
    "# Helper method to play with the metric\n",
    "def pg_trgm_similarity(text, other_text):\n",
    "    cursor.execute('SELECT similarity(%s, %s);', (text, other_text))\n",
    "    similarity = cursor.fetchall()[0][0]\n",
    "    return similarity\n",
    "                   \n",
    "# TEST\n",
    "print(pg_trgm_similarity('hi', 'hey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9fb90ca4ee469a86f7173f5dde0017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../notebooks/Simple QA Numbers/HYPOTHESIS - Subject Name not in Question.ipynb\n",
      "\n",
      "Answerable Precision: 0.967804 [20471 of 21152]\n",
      "Answerable Recall: 0.998301 [21152 of 21188]\n",
      "Expected Guessing Accuracy: 0.639845 [13825 of 21607]\n",
      "Negative Sample:\n",
      "| Index | Strategy | Max Similarity | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| 0 | PREPROCESSED | 0.0 | ken hirai | album | ['album'] | what is the cover album by Japanese pop singer Ken Hirai |\n",
      "| 1 | PREPROCESSED | 0.714286 | st helens rfc | st helens | ['st helens rlfc', 'st helens'] | What is a color of the st helens rlfc |\n",
      "| 2 | PREPROCESSED | 0.0 | thriller | felix enriquez alcala | ['felix enriquez alcala'] | what thriller was directed by Félix Enríquez Alcalá |\n",
      "| 3 | PREPROCESSED | 1.0 | who 's that girl ? | who s that girl | ['who s that girl'] | what track came off  whos that girl? |\n",
      "| 4 | PREPROCESSED | 0.0 | black - and - white | drama film | ['drama film'] | What is the drama film where black-and-white technique was used? |\n",
      "| 5 | PREPROCESSED | 0.777778 | aniruddha roy chowdhury | chowdhury | ['aninuddha roy chowdhury', \"aninuddha roy chowdhury 's\", 'roy chowdhury', 'is aninuddha roy chowdhury', 'aninuddha roy', 'chowdhury'] | What is aninuddha roy chowdhury's profession? |\n",
      "| 6 | PREPROCESSED | 0.0333333 | scandinavian airlines | accident | ['accident'] | what scandinavian airlines flight was involved in an accident |\n",
      "| 7 | PREPROCESSED | 0.615385 | adeventure | adventure | ['adventure'] | What is an adventure cvg game? |\n",
      "| 8 | PREPROCESSED | 0.846154 | 2010 fantasia film festival | 2010 | ['2010 fantasia festival', '2010 fantasia', 'fantasia festival', 'the 2010 fantasia festival', '2010'] | What's a movie that was shown at the 2010 fantasia festival |\n",
      "| 9 | PREPROCESSED | 0.0 | parody | american | ['american'] | name a 2007 American film parody  |\n",
      "| 10 | PREPROCESSED | 0.454545 | jane child | jane | ['jane'] | is jane child pop music or j-pop |\n",
      "| 11 | PREPROCESSED | 0.309524 | entrepreneur | and | ['opens and runs restaurants', 'opens and runs', 'opens and runs restaurants professionally', 'and runs restaurants', 'who opens and runs restaurants', 'opens and', 'and runs', 'who opens and runs', 'runs restaurants', 'opens', 'and runs restaurants professionally', 'who opens and runs restaurants professionally', 'runs', 'entrepreneur who opens and runs restaurants', 'and'] | what is the tittle of an entrepreneur who opens and runs restaurants professionally |\n",
      "| 12 | PREPROCESSED | 0.4375 | music for madame | madame | ['madame'] | where is the movie music for madame from |\n",
      "| 13 | PREPROCESSED | 0.0 | liquid | hydrogen peroxide | ['hydrogen peroxide formula', 'peroxide formula', 'hydrogen peroxide'] | What's a hydrogen peroxide formula available in liquid |\n",
      "| 14 | PREPROCESSED | 0.842105 | historical novel | titles | ['historical novels', 'historical', 'novels', 'of historical novels', 'of historical', 'titles of historical novels', 'of', 'titles'] | what are titles of historical novels |\n",
      "| 15 | NORMALIZED_PUNCTUATION | 0.826087 | whiskey , mystics and men | & men | ['whiskey , mystics & men', ', mystics & men', 'wrote whiskey , mystics & men', 'whiskey , mystics &', 'mystics & men', 'whiskey , mystics', ', mystics &', 'wrote whiskey , mystics &', 'who wrote whiskey , mystics & men', '& men'] | who wrote whiskey, mystics & men |\n",
      "| 16 | PREPROCESSED | 0.714286 | john binden | john binder | ['john binder'] | john binder identifies with what gender? |\n",
      "| 17 | PREPROCESSED | 0.478261 | irene born | the daughter | ['the daughter of irene', 'daughter of irene', 'the daughter of irene born', 'was the daughter of irene', 'the daughter of', 'of irene', 'daughter of irene born', 'daughter of', 'the daughter'] | which actress was the daughter of irene born |\n",
      "| 18 | PREPROCESSED | 0.823529 | michael noland | michael | ['michael nolands', 'michael nolands profession', 'nolands', 'is michael nolands', 'michael'] | what is  michael nolands profession |\n",
      "| 19 | PREPROCESSED | 0.0 | album | the bee gees | ['the bee gees'] | what is the album by the Bee Gees released on 1969 |\n",
      "| 20 | PREPROCESSED | 1.0 | r.j . 's latest arrival | arrival | ['r.j.s latest arrival', 'latest arrival', 'r.j.s latest arrival is', 'r.j.s latest', 'arrival'] | r.j.s latest arrival is featured on what label? |\n",
      "| 21 | PREPROCESSED | 0.7 | bengalis | bengali | ['bengali'] | Who is someone that is from the bengali people |\n",
      "| 22 | PREPROCESSED | 0.0 | state of palestine | midfielder | ['midfielder'] | Who's a defensive midfielder born in state of palestine |\n",
      "| 23 | PREPROCESSED | 0.0769231 | flight | female | ['female'] | Name a female fictional character with the flight ability. |\n",
      "| 24 | PREPROCESSED | 0.6875 | i 'll remember | ill | ['ill remember', 'ill'] | What kind a music form is ill remember |\n",
      "| 25 | PREPROCESSED | 1.0 | she 's like the wind | she s like the wind | ['she s like the wind'] | what is the recording with the self titled track shes like the wind |\n",
      "| 26 | PREPROCESSED | 0.8 | jeff beaucar | jeff | ['jeff beaucars', 'beaucars', 'jeff beaucars profession', 'is jeff beaucars', 'jeff'] | what is jeff beaucars profession |\n",
      "| 27 | PREPROCESSED | 0.0 | chicago | live album | ['live album'] | Name a live album by chicago. |\n",
      "| 28 | PREPROCESSED | 0.764706 | it 's in your eyes | in your eyes | ['its in your eyes', 'in your eyes'] | in what form is the composition its in your eyes |\n",
      "| 29 | PREPROCESSED | 0.818182 | the westward movement | westward | ['westward movement', 'westward'] | which subject was focused on in  the westward movement |\n",
      "| 30 | PREPROCESSED | 0.869565 | charles iv of hungary | charles | ['charles iv of hungar', \"charles iv of hungar 's\", 'iv of hungar', 'is charles iv of hungar', 'charles iv of', \"charles iv of hungar 's kingdom\", 'of hungar', 'charles iv', \"iv of hungar 's\", \"is charles iv of hungar 's\", 'iv of', 'what is charles iv of hungar', 'is charles iv of', 'hungar', 'charles'] | what is charles iv of hungar's kingdom  |\n",
      "| 31 | PREPROCESSED | 0.652174 | william morgan shuster | william morgan | ['william morgan'] | which New York city did william morgan shuster in |\n",
      "| 32 | PREPROCESSED | 0.571429 | drums | drum | ['drum'] | Who plays the drum kit? |\n",
      "| 33 | PREPROCESSED | 0.8 | notte prima degli esami | notte prima degli esami ( disc 1 ) | ['notte prima degli esami ( disc 1 )'] | What format was notte prima degli esami (disc 1) released in? |\n",
      "| 34 | PREPROCESSED | 0.0 | album | koda kumi | ['koda kumi'] | what is the name of an album by Koda Kumi |\n",
      "| 35 | PREPROCESSED | 0.611111 | sem limite | sem limite ( disc 1 ) | ['sem limite ( disc 1 )'] | which artist recorded the album sem limite (disc 1)? |\n",
      "| 36 | PREPROCESSED | 0.421053 | the english ritual for knights templar | knights templar | ['knights templar'] | who worte the english ritual for knights templar |\n",
      "| 37 | NORMALIZED_PUNCTUATION | 0.217391 | dido | dido ( queen of carthage ) | ['dido ( queen of carthage )'] | where did dido (queen of carthage) die? |\n",
      "| 38 | PREPROCESSED | 0.857143 | susan helms | helms | ['susan j. helms', 'mission susan j. helms', 'j. helms', 'susan j.', 'susan j. helms went', 'a mission susan j. helms', 'helms'] | What is a mission susan j. helms went on? |\n",
      "| 39 | PREPROCESSED | 0.0 | album | tony bennett | ['tony bennett'] | what is an album by Tony Bennett released on 2011 |\n",
      "| 40 | PREPROCESSED | 0.571429 | drums | drum | ['drum'] | What instrumentalist is known for using a drum kit? |\n",
      "| 41 | PREPROCESSED | 0.583333 | into africa | africa | ['africa'] | What zoo offers the exhibit into africa |\n",
      "| 42 | PREPROCESSED | 0.461538 | punjab , india | india | ['india'] | Which is an educational institution for management located in punjab, india? |\n",
      "| 43 | PREPROCESSED | 1.0 | deep inside ... | deep inside | ['deep inside'] | What format was deep inside... released in |\n",
      "| 44 | PREPROCESSED | 0.761905 | dr . wonder 's workshop | dr . | ['dr . wonders workshop', '. wonders workshop', 'dr . wonders workshop originally', 'was dr . wonders workshop', 'dr . wonders', 'wonders workshop', 'dr . wonders workshop originally from', '. wonders workshop originally', 'dr .'] | where was dr. wonders workshop originally from |\n",
      "| 45 | PREPROCESSED | 0.666667 | monarch | the monarch | ['the monarch'] | what is the album recorded by the monarch |\n",
      "| 46 | PREPROCESSED | 0.0 | song | dj fresh | ['dj fresh and diplo', 'fresh and diplo', 'dj fresh and', 'by dj fresh and diplo', 'and diplo', 'fresh and', 'dj fresh'] | what is the name of a  song by DJ fresh and Diplo |\n",
      "| 47 | PREPROCESSED | 0.814815 | papua new guinea time zone | papua new guinea | ['papua new guinea time', 'papua new guinea'] | What is the name of a country located in the  papua new guinea time |\n",
      "| 48 | PREPROCESSED | 0.571429 | theme from er | er | ['from er', 'er'] | Who produced the theme from er |\n",
      "| 49 | PREPROCESSED | 0.0526316 | farmer | horse | ['horse trader', 'horse'] | Who's a farmer that also worked as a horse trader |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.table import format_pipe_table\n",
    "\n",
    "negative_samples = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        strategy = 'PREPROCESSED'\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            if row['subject'] not in candidates_mids[-1]:\n",
    "                considered_aliases = [predicted['name'] for j, predicted in \n",
    "                                          enumerate(row['predicted_subject_names']) if j <= i]\n",
    "                negative_samples.append({\n",
    "                    'Preprocessed Subject Name': text_preprocess(row['subject_name']),\n",
    "                    'Considered Aliases': considered_aliases,\n",
    "                    'Max Similarity': max([pg_trgm_similarity(row['subject_name'], a)\n",
    "                                           for a in considered_aliases]),\n",
    "                    'Predicted Alias': predicted['name'],\n",
    "                    'Strategy': strategy,\n",
    "                    'Question': row['question'],\n",
    "                })\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Negative Sample:')\n",
    "print(format_pipe_table(negative_samples[:50], columns=['Strategy', 'Max Similarity',\n",
    "                                                        'Preprocessed Subject Name',\n",
    "                                                        'Predicted Alias',\n",
    "                                                        'Considered Aliases', 'Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "\n",
    "Recall increased by 0.001784.\n",
    "Precision increased by 0.004104.\n",
    "\n",
    "\n",
    "##### Error Bucket:\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Handling possesives would fix 10 / 50 errors. Handling the `Similar` bucket would be difficult because it's typically because of extra words in the subject name not present in the question.\n",
    "\n",
    "**Buckets:**\n",
    "- Wrong Span (29 / 50): The wrong span in the question was selected\n",
    "- Suffix (12 / 50): The correct subject name was not linked due to a suffix.\n",
    "- Extra Article (3 / 50): The correct subject name was not linked due to an article.\n",
    "- Similar (7 / 50): The correct subject name was similar but not exact to the predicted subject name.\n",
    "- Other (1 / 50): Deeper reason that the correct subject name was not linked.\n",
    "\n",
    "| Index | Similarity | Bucket | Strategy | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 0.0 | Wrong Span | PREPROCESSED | short | documentary film | ['documentary film'] | Name a short documentary film released in 2011 |\n",
    "| 1 | 0.722222 | Suffix | PREPROCESSED | red cloud 's war | the red | ['the red clouds war', 'red clouds war', 'the red clouds', 'clouds war', 'red clouds', 'the red'] | what was involved in the red clouds war? |\n",
    "| 2 | 0.75 | Wrong Span | PREPROCESSED | corporation nation | nation | ['nation book', 'corporation nation book', 'the corporation nation book', 'nation'] | what subject is the corporation nation book about |\n",
    "| 3 | 0.8 | Suffix | PREPROCESSED | peter 's point plantation | peters | ['peters point plantation', 'peters point', 'point plantation', 'peters'] | What is peters point plantation's architectural style |\n",
    "| 4 | 0.0555556 | Wrong Span | PREPROCESSED | album | aaron carter | ['aaron carter'] | Name an album released by aaron carter |\n",
    "| 5 | 1.0 | Similar | PREPROCESSED | pillows & prayers : cherry red 1982–1983 | pillows & prayers : cherry red 1982 - 1983 | ['pillows & prayers : cherry red 1982 - 1983'] | What is the name of the track list for the release pillows & prayers: cherry red 1982-1983? |\n",
    "| 6 | 0.5 | Wrong Span | PREPROCESSED | commune of luxembourg | luxembourg | ['luxembourg'] | which country is the commune of luxembourg in |\n",
    "| 7 | 0.764706 | Extra Article | PREPROCESSED | the hits album 6 | 6 | ['hits album 6', '6'] | what song was included in the hits album 6 |\n",
    "| 8 | 0.588235 | Wrong Span | PREPROCESSED | between two women | two women | ['two women'] | what is about between two women |\n",
    "| 9 | 0.782609 | Suffix | PREPROCESSED | battle of hudson 's bay | bay | ['battle of hudsons bay', 'of hudsons bay', 'battle of hudsons', 'the battle of hudsons bay', 'hudsons bay', 'did the battle of hudsons bay', 'battle of', 'bay'] | where did the battle of hudsons bay take place |\n",
    "| 10 | 0.0 | Wrong Span | PREPROCESSED | tablet | hypertension | ['hypertension'] | what is a tablet used to treat hypertension  |\n",
    "| 11 | 0.0 | Wrong Span | PREPROCESSED | compilation album | frank zappa | ['frank zappa'] | what compilation album did frank zappa release? |\n",
    "| 12 | 0.0 | Wrong Span | PREPROCESSED | soundtrack | anthony marinelli | ['anthony marinelli'] | What's a soundtrack written by anthony marinelli |\n",
    "| 13 | 0.0 | Wrong Span | PREPROCESSED | album | george canyon | ['george canyon'] | name an album by George Canyon |\n",
    "| 14 | 0.0 | Wrong Span | PREPROCESSED | album | portal | ['portal'] | What's an album by the band portal |\n",
    "| 15 | 0.785714 | Suffix | PREPROCESSED | megan cheng | megan | ['megan chengs', 'megan'] | whats  megan chengs ethnicity |\n",
    "| 16 | 0.705882 | Wrong Span | PREPROCESSED | martial arts film | martial arts | ['martial arts'] | what is the name of the netflix martial arts film? |\n",
    "| 17 | 0.0222222 | Wrong Span | PREPROCESSED | creedence clearwater revival | compilation album | ['compilation album'] | What is a compilation album by creedence clearwater revival |\n",
    "| 18 | 0.227273 | Wrong Span | PREPROCESSED | topical medication | medicine | ['medicine'] | Name a topical medicine |\n",
    "| 19 | 0.636364 | Wrong Span | PREPROCESSED | master | the master | ['the master'] | what is one of the master's powers  |\n",
    "| 20 | 0.0 | Other | PREPROCESSED | t - town | kearny | ['kearny'] | What newspaper circulates in the town of kearny |\n",
    "| 21 | 0.571429 | Suffix | PREPROCESSED | drums | drum | ['drum'] | which musician plays the drum kit |\n",
    "| 22 | 0.84375 | Suffix | PREPROCESSED | dimillo 's floating restaurant | restaurant | ['dimillos floating restaurant', 'dimillos floating', 'floating restaurant', 'dimillos', 'is dimillos floating restaurant', 'dimillos floating restaurant in', 'restaurant'] | what state is dimillos floating restaurant in? |\n",
    "| 23 | 0.0 | Wrong Span | PREPROCESSED | ragtime | denmark | ['denmark'] | who is the ragtime artist born in denmark? |\n",
    "| 24 | 0.8 | Similar, Extra Article | PREPROCESSED | the regatta mystery | mystery | ['regatta mystery', 'mystery'] | what theme is in the piece regatta mystery |\n",
    "| 25 | 0.0 | Wrong Span | PREPROCESSED | album | jack | ['jack dejohnrette', 'jack'] | What is the name of Jack DeJohnrette's album? |\n",
    "| 26 | 0.0 | Wrong Span | PREPROCESSED | bollywood | tamil | ['tamil'] | what bollywood Tamil film was released in 2004  |\n",
    "| 27 | 0.0 | Wrong Span | PREPROCESSED | animated cartoon | ducks | ['ducks'] | what animated cartoon was about ducks? |\n",
    "| 28 | 0.0 | Wrong Span | PREPROCESSED | photography | visual art | ['visual art'] | which artist uses photography as their preferred visual art form |\n",
    "| 29 | 0.761905 | Suffix | PREPROCESSED | this pud 's for you | for you | ['this puds for you comes', 'this puds for you', 'this puds for you comes from', 'puds for you comes', 'puds for you', 'this puds for', 'this puds', 'episode this puds for you comes', 'for you comes', 'puds for you comes from', 'episode this puds for you', 'for you'] | what is the series where the episode this puds for you comes from |\n",
    "| 30 | 0.826087 | Suffix | NORMALIZED | chet 's speech , part ii | , part ii | ['chets speech , part ii', 'speech , part ii', 'chets speech , part', 'chets speech ,', 'chets speech', ', part ii'] | who sings chets speech, part ii |\n",
    "| 31 | 0.764706 | Wrong Span | PREPROCESSED | large family car | family | ['large family', 'family'] | What car model is an example of a large family car? |\n",
    "| 32 | 0.761905 | Suffix | PREPROCESSED | men 's pommel horse | pommel horse | ['mens pommel horse', 'mens pommel', 'pommel horse'] | What olympic games featured mens pommel horse |\n",
    "| 33 | 0.0526316 | Wrong Span | NORMALIZED | soundtrack | s.cry.ed | ['s.cry.ed'] | What's the soundtrack for s.cry.ed |\n",
    "| 34 | 0.35 | Wrong Span | PREPROCESSED | sahara ( instrumental ) | sahara | ['sahara'] | who composed sahara (instrumental)? |\n",
    "| 35 | 0.0625 | Wrong Span | PREPROCESSED | compilation | cema | ['albumby cema', 'cema'] | what album is released as a compilation albumby CEMA |\n",
    "| 36 | 0.583333 | Wrong Span | PREPROCESSED | arabic name | arabic | ['arabic'] | What is a book that is about arabic name |\n",
    "| 37 | 0.73913 | Similar | PREPROCESSED | multiplayer video game | game | ['multiplayer game', 'game'] | What's a text based multiplayer game |\n",
    "| 38 | 0.75 | Extra Article | PREPROCESSED | the crystal city | crystal city | ['crystal city'] | what genre is crystal city |\n",
    "| 39 | 0.84 | Suffix | PREPROCESSED | men 's badminton , singles | singles | ['mens badminton , singles', 'badminton , singles', 'mens badminton', 'mens badminton ,', 'singles'] | what olympic games was mens badminton, singles apart of |\n",
    "| 40 | 0.0 | Wrong Span | PREPROCESSED | mercedes lackey | fantasy | ['fantasy'] | which fantasy series were written by mercedes lackey? |\n",
    "| 41 | 0.666667 | Similar | PREPROCESSED | brian o'shea | brian oshea | ['brian oshea'] | brian oshea performs what type of martial art |\n",
    "| 42 | 0.857143 | Similar | PREPROCESSED | u.s . office of war information | war | ['office of war information', 'office of war information help', 'the office of war information', 'war information', 'the office of war information help', 'of war information', 'office of war', 'war information help', 'of war information help', 'the office of war', 'office of war information help produce', 'war'] | which film did the office of war information help produce  |\n",
    "| 43 | 0.0 | Wrong Span | PREPROCESSED | album | sham 69 | ['sham 69'] | which album is released by Sham 69 |\n",
    "| 44 | 0.777778 | Wrong Span | NORMALIZED | lowthian bell | , 1st baronet | ['sir lowthian bell , 1st baronet', 'lowthian bell , 1st baronet', 'sir lowthian bell , 1st', 'sir lowthian bell ,', 'sir lowthian bell', 'bell , 1st baronet', 'sir lowthian', ', 1st baronet'] | what organization was founded by sir lowthian bell, 1st baronet |\n",
    "| 45 | 0.862069 | Suffix | PREPROCESSED | st . peter 's episcopal church | st . peters | ['st . peters episcopal church', 'st . peters episcopal', '. peters episcopal church', 'peters episcopal church', 'st . peters'] | what state and city is st. peters episcopal church located in? |\n",
    "| 46 | 0.851852 | Suffix | PREPROCESSED | richard scarry 's busytown | busytown | ['richard scarrys busytown', 'scarrys busytown', 'richard scarrys', 'busytown'] | what is a gameplay mode featured on richard scarrys busytown |\n",
    "| 47 | 0.0 | Wrong Span | PREPROCESSED | album | soil | ['soil'] | What's an album by soil |\n",
    "| 48 | 0.714286 | Similar | PREPROCESSED | texas a&m university school of law | texas wesleyan university | ['texas wesleyan university school of law', 'wesleyan university school of law', 'texas wesleyan university school of', 'texas wesleyan university school', 'university school of law', 'is texas wesleyan university school of law', 'texas wesleyan university'] | Where is texas wesleyan university school of law located? |\n",
    "| 49 | 0.535714 | Wrong Span | PREPROCESSED | public service announcement | public service | ['public service'] | What is the name of a public service announcement? |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2\n",
    "\n",
    "In version 1, the error bucketing revealed a failure to handle suffix's; therefore, we proceed to handle them in version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f3d9a24c1946c9a0f7ed06e204fcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answerable Precision: 0.972742 [20591 of 21168]\n",
      "Answerable Recall: 0.999056 [21168 of 21188]\n",
      "Expected Guessing Accuracy: 0.644360 [13922 of 21607]\n",
      "Negative Sample:\n",
      "| Index | Strategy | Max Similarity | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| 0 | PREPROCESSED | 0.0 | armenians | novelist | ['novelist'] | who is a famous armenians novelist and playwright  |\n",
      "| 1 | PREPROCESSED | 0.238095 | jojo 's bizarre adventure | jojo | ['jojo'] | Who is a character of jojo's bizarre adventure? |\n",
      "| 2 | PREPROCESSED | 0.0 | seychelles | praslin | ['praslin'] | What is a administrative district in seychelles that is located on the island of Praslin? |\n",
      "| 3 | PREPROCESSED | 0.0 | album | elmore james | ['the history of elmore james', 'history of elmore james', 'of elmore james', 'elmore james'] | what is an album title including the history of elmore james |\n",
      "| 4 | PREPROCESSED | 1.0 | crime dramas | crime drama | ['crime drama'] | what is a crime drama film on netflix? |\n",
      "| 5 | PREPROCESSED | 0.184211 | scream | scream ( feat . keri hilson & nicole scherzinger ) | ['scream ( feat . keri hilson & nicole scherzinger )'] | what content type is scream (feat. keri hilson & nicole scherzinger)? |\n",
      "| 6 | PREPROCESSED | 0.0 | palomar observatory | asteroid | ['asteroid'] | what asteroid was discovered at the palomar observatory |\n",
      "| 7 | PREPROCESSED | 1.0 | mrs brown | mrs . brown | ['mrs . brown'] | mrs. brown was executive produced by who |\n",
      "| 8 | PREPROCESSED | 0.0 | anarchism | tolstoy | ['tolstoy'] | which tolstoy book is based in anarchism? |\n",
      "| 9 | PREPROCESSED | 0.0 | marvel universe | female | ['female'] | What is the name of a  female marvel universe character? |\n",
      "| 10 | PREPROCESSED | 0.333333 | the techniques | the | ['the'] | is the techniques reggae or rock |\n",
      "| 11 | PREPROCESSED | 0.666667 | dan laustsen | dan | ['dan lausten', 'dan lausten given', 'lausten', 'dan'] | what film is dan lausten given credit as cinematographer? |\n",
      "| 12 | PREPROCESSED | 0.466667 | killed in action | action | ['action'] | which english poet was killed in action? |\n",
      "| 13 | PREPROCESSED | 0.6 | h. w. olbers | olbers | ['heinrich wilhelm olbers', 'wilhelm olbers', 'heinrich wilhelm', 'of heinrich wilhelm olbers', 'olbers'] | what is the nationality of heinrich wilhelm olbers |\n",
      "| 14 | PREPROCESSED | 0.0 | album | thad jones | ['thad jones'] | what album did thad jones release in 1957? |\n",
      "| 15 | PREPROCESSED | 1.0 | tyler greene | tyler green | ['tyler green'] | Where was Tyler Green born? |\n",
      "| 16 | PREPROCESSED | 0.0 | plymouth | secondary school | ['secondary school'] | what's a post secondary school in plymouth |\n",
      "| 17 | PREPROCESSED | 0.0 | actor | new york | ['new york'] | who is a professional actor that was born in New York |\n",
      "| 18 | NORMALIZED_PUNCTUATION_STEM | 0.625 | philosophy | covering | ['covering philosophy', 'covering'] | WHat's a book covering philosophy |\n",
      "| 19 | NORMALIZED_PUNCTUATION_STEM | 0.78125 | monty python 's life of brian | 's life of brian | [\"wasmonty python 's life of brian\", \"python 's life of brian\", \"wasmonty python 's life of\", \"release wasmonty python 's life of brian\", \"'s life of brian\"] | what kind of release wasmonty python's life of brian |\n",
      "| 20 | PREPROCESSED | 0.277778 | philosophy of mind | mind | ['mind'] | who has philosophy of mind |\n",
      "| 21 | PREPROCESSED | 0.5 | penguin exhibit | penguin | ['penguin'] | what species is in the penguin exhibit? |\n",
      "| 22 | PREPROCESSED | 0.0 | soundtrack | romantic comedy | ['a british romantic comedy', 'british romantic comedy', 'a british romantic', 'of a british romantic comedy', 'romantic comedy'] | what is the soundtrack of a british romantic comedy |\n",
      "| 23 | NORMALIZED_PUNCTUATION | 0.875 | \\\"you'll be in my heart\\ \" spanish version | you ll be in my heart | ['you ll be in my heart spanish version', 'll be in my heart spanish version', 'you ll be in my heart spanish', 'you ll be in my heart spanish version ?', 'be in my heart spanish version', 'composed you ll be in my heart spanish version', 'll be in my heart spanish', 'll be in my heart spanish version ?', 'you ll be in my heart'] | who composed youll be in my heart spanish version? |\n",
      "| 24 | PREPROCESSED | 0.0 | dag nasty | album | ['album'] | What's an album featuring dag nasty |\n",
      "| 25 | PREPROCESSED | 0.0 | local | detroit | ['detroit'] | what's a local neighborhood in detroit |\n",
      "| 26 | PREPROCESSED | 0.5 | wedding dress | wedding | ['wedding'] | What's a film that is about a wedding dress |\n",
      "| 27 | PREPROCESSED | 0.714286 | multiplayer video game | game | ['multiplayer game', 'game'] | what is a game that has a multiplayer game mode? |\n",
      "| 28 | PREPROCESSED | 0.0 | album | 1995 | ['1995'] | what is an album from 1995 |\n",
      "| 29 | PREPROCESSED | 0.0 | album | lou barlow | ['lou barlow'] | what is an album by Lou Barlow? |\n",
      "| 30 | PREPROCESSED | 0.409091 | business process outsourcing | outsourcing | ['outsourcing industry', 'outsourcing'] | what company is in the business process outsourcing industry? |\n",
      "| 31 | PREPROCESSED | 0.861111 | house committee on the judiciary | united states | ['united states house committee', 'united states house committee on', 'united states house committee on the', 'united states house committee on the judiciary', 'united states house', 'states house committee', 'the united states house committee', 'states house committee on', 'states house committee on the', 'the united states house committee on', 'the united states house committee on the', 'states house committee on the judiciary', 'the united states house committee on the judiciary', 'united states'] | who is one of the members of the united states house committee on the judiciary |\n",
      "| 32 | PREPROCESSED | 0.0 | album | patrick watson | ['patrick watson'] | what is an album by Patrick Watson? |\n",
      "| 33 | PREPROCESSED | 0.0 | preschool | hong kong | ['hong kong'] | What's a preschool in hong kong |\n",
      "| 34 | PREPROCESSED | 0.0 | theodore j. pahle | drama film | ['drama film'] | Name a drama film by theodore j. pahle. |\n",
      "| 35 | PREPROCESSED | 0.0 | album | the crusaders | ['the crusaders'] | Name an album  by The Crusaders |\n",
      "| 36 | PREPROCESSED | 1.0 | painting | paintings | ['paintings'] | which famous spanish artist is known for his paintings? |\n",
      "| 37 | PREPROCESSED | 0.6875 | phil parsons racing | phil | ['phil parsons', 'phil'] | Name a driver in the Phil Parsons Race.  |\n",
      "| 38 | PREPROCESSED | 0.769231 | burning times | the burning times | ['the burning times'] | Which country is the burning times released in |\n",
      "| 39 | NORMALIZED_PUNCTUATION | 0.863636 | abdel aziz al - rantisi | al - | ['abdel aziz al - rantissi', \"abdel aziz al - rantissi 's\", 'aziz al - rantissi', 'is abdel aziz al - rantissi', 'abdel aziz al -', 'al - rantissi', \"abdel aziz al - rantissi 's ethnicity\", \"aziz al - rantissi 's\", 'abdel aziz al', \"is abdel aziz al - rantissi 's\", 'aziz al -', 'what is abdel aziz al - rantissi', 'is abdel aziz al -', '- rantissi', \"al - rantissi 's\", \"aziz al - rantissi 's ethnicity\", 'abdel aziz', \"is abdel aziz al - rantissi 's ethnicity\", 'al -'] | what is abdel aziz al-rantissi's ethnicity  |\n",
      "| 40 | PREPROCESSED | 0.333333 | costume drama | drama film | ['drama film'] | what is a costume drama film  |\n",
      "| 41 | PREPROCESSED | 0.0 | ecuador | volcano | ['volcano'] | Name a Volcano in Ecuador.  |\n",
      "| 42 | PREPROCESSED | 0.714286 | multiplayer video game | game | ['multiplayer game', 'game'] | what's an example of a multiplayer game? |\n",
      "| 43 | NORMALIZED_PUNCTUATION_STEM | 0.454545 | location awareness | awareness | ['awareness'] | what is a software that uses location awareness |\n",
      "| 44 | PREPROCESSED | 0.555556 | the pier | pier | ['pier'] | What is the pier's film format? |\n",
      "| 45 | NORMALIZED_PUNCTUATION | 0.75 | l. l. marshall | l.l | ['l.l marshall', 'l.l'] | What gender is L.L Marshall? |\n",
      "| 46 | PREPROCESSED | 1.0 | album | albums | ['albums'] | which albums were released by a japanese pop-rock band? |\n",
      "| 47 | NORMALIZED_PUNCTUATION | 0.714286 | 1963 - 2003 : 40th anniversary collection | 40th anniversary collection ( | ['40th anniversary collection ( disc 2 )', '40th anniversary collection ( disc 2', 'anniversary collection ( disc 2 )', '40th anniversary collection ( disc 2 ) ?', 'of 40th anniversary collection ( disc 2 )', '40th anniversary collection ( disc', '40th anniversary collection ('] | what is the album content type of 40th anniversary collection (disc 2)? |\n",
      "| 48 | PREPROCESSED | 0.571429 | defenseman | hockey | ['defenceman', 'playing defenceman', 'defenceman (', 'from playing defenceman', 'defenceman ( ice', 'playing defenceman (', 'hockey'] | who retired from playing defenceman (ice hockey)? |\n",
      "| 49 | PREPROCESSED | 0.8 | the blackstone group | blackstone | ['blackstone group', 'business blackstone group', 'blackstone'] | what industry is the business  blackstone group apart of |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.table import format_pipe_table\n",
    "\n",
    "negative_samples = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        strategy = 'PREPROCESSED'\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION_STEM'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            if row['subject'] not in candidates_mids[-1]:\n",
    "                considered_aliases = [predicted['name'] for j, predicted in \n",
    "                                          enumerate(row['predicted_subject_names']) if j <= i]\n",
    "                negative_samples.append({\n",
    "                    'Preprocessed Subject Name': text_preprocess(row['subject_name']),\n",
    "                    'Considered Aliases': considered_aliases,\n",
    "                    'Max Similarity': max([pg_trgm_similarity(text_normalize_punctuation_stem(row['subject_name']),\n",
    "                                                              text_normalize_punctuation_stem(a))\n",
    "                                           for a in considered_aliases]),\n",
    "                    'Predicted Alias': predicted['name'],\n",
    "                    'Strategy': strategy,\n",
    "                    'Question': row['question'],\n",
    "                })\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Negative Sample:')\n",
    "# To not overfit on the first 50 samples\n",
    "print(format_pipe_table(negative_samples[50:100], columns=['Strategy', 'Max Similarity',\n",
    "                                                        'Preprocessed Subject Name',\n",
    "                                                        'Predicted Alias',\n",
    "                                                        'Considered Aliases', 'Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "\n",
    "Recall increased by 0.000376.\n",
    "\n",
    "Precision increased by 0.004896.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "We managed to increase our expected accuracy by a 1% with these simple normalization measures. We believe this will lead to a 1% + gain downstream.\n",
    "\n",
    "##### Error Bucket:\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "For the most part, the wrong span is selected; therefore, the alias is difficult to link with the current implementation of the algorithm.\n",
    "\n",
    "We can handle the similar case some what. The similarity between the correct alias and the subject name tends to be fairly high; therefore, we can try adding a step to look for any aliases that are similar with a score of 0.85+. We expect this to handle  3 / 50 errors.\n",
    "\n",
    "| Max Similarity | Bucket |\n",
    "| --- | --- |\n",
    "| 0.888889 | Similar |\n",
    "| 0.714286 | Similar |\n",
    "| 0.875 | Similar |\n",
    "| 1.0 | Similar |\n",
    "| 0.62069 | Similar |\n",
    "| 1.0 | Similar |\n",
    "| 0.88 | Similar |\n",
    "\n",
    "**Buckets:**\n",
    "- Wrong Span (42 / 50): The wrong span in the question was selected.\n",
    "- Similar (8 / 50): The correct subject name was similar but not exact to the predicted subject name.\n",
    "- Extra Article (2 / 50): The correct subject name was not linked due to an article.\n",
    "\n",
    "| Index | Max Similarity  | Bucket | Strategy | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 0.705882 | Wrong Span | PREPROCESSED | pim fortuyn list | pim fortuyn | ['pim fortuyn'] | what ideology does the pim fortuyn list follow |\n",
    "| 1 | 0.0 | Wrong Span | NORMALIZED_PUNCTUATION_STEM | drug | cleaning | ['cleaning hands', 'for cleaning hands', 'cleaning'] | what drug is used for cleaning hands  |\n",
    "| 2 | 0.444444 | Wrong Span | PREPROCESSED | leather subculture | leather | ['leather'] | what's one event that celebrates the leather subculture |\n",
    "| 3 | 0.333333 | Wrong Span | PREPROCESSED | illinois river | rogue river | ['lower rogue river', 'rogue river'] | in which community does the illinois river confluence with the Lower Rogue river |\n",
    "| 4 | 0.764706 | Wrong Span, Extra Article | PREPROCESSED | need for speed | the need for speed | ['the need for speed'] | what type of film is the need for speed |\n",
    "| 5 | 0.0 | Wrong Span | PREPROCESSED | asteroid | geologist | ['geologist'] | which asteroid was names after an italian geologist? |\n",
    "| 6 | 0.0 | Wrong Span | PREPROCESSED | album | chico debarge | ['chico debarge'] | what is an album by Chico DeBarge? |\n",
    "| 7 | 1.0 | Punctuation | PREPROCESSED | unter null . | unter null | ['unter null'] | what type of book binding is unter null. |\n",
    "| 8 | 0.5 | Wrong Span | NORMALIZED_PUNCTUATION | k - pop | - pop music | ['k - pop music', '- pop music'] | Who is an artist of k-pop music? |\n",
    "| 9 | 0.0454545 | Wrong Span | PREPROCESSED | novel | vladimir nabokov | ['vladimir nabokov'] | Name a novel by Vladimir Nabokov |\n",
    "| 10 | 0.888889 | Similar | PREPROCESSED | millard s drexler | millard | ['millard drexler', 'drexler', 'millard'] | What organization did millard drexler found |\n",
    "| 11 | 0.0 | Wrong Span | PREPROCESSED | musical | israel | ['israel'] | which musical films were broadcasted in israel? |\n",
    "| 12 | 0.0 | Wrong Span | PREPROCESSED | live album | 3oh!3 | ['3oh!3'] | what is the name of the live album by 3oh!3 |\n",
    "| 13 | 0.5 | Wrong Span | PREPROCESSED | epic film | epic | ['epic'] | Name a 1936 epic film  |\n",
    "| 14 | 0.533333 | Wrong Span | PREPROCESSED | roy rogers restaurants | roy rogers | ['roy rogers'] | roy rogers restaurants in which industry? |\n",
    "| 15 | 0.0 | Wrong Span | PREPROCESSED | caucasian | babylon 5 | ['babylon 5'] | who is of caucasian race in babylon 5 |\n",
    "| 16 | 0.6 | Wrong Span | PREPROCESSED | painting | visual art | ['visual art painting', 'art painting', 'visual art'] | what is a visual art painting |\n",
    "| 17 | 0.0 | Wrong Span | PREPROCESSED | album | funk | ['koul funk', 'funk'] | what album was release by Koul Funk |\n",
    "| 18 | 0.0555556 | Wrong Span | PREPROCESSED | science | brian swimme | ['brian swimme'] | brian swimme wrote what book that dealt with  science |\n",
    "| 19 | 0.0 | Wrong Span | PREPROCESSED | animation | raoul servais | ['raoul servais'] | Which animation did Raoul Servais direct |\n",
    "| 20 | 0.333333 | Wrong Span | PREPROCESSED | working title films | films | ['films'] | what is the film from the production company working title films |\n",
    "| 21 | 0.0344828 | Wrong Span | PREPROCESSED | jerry bruckheimer | biographical film | ['biographical film'] | jerry bruckheimer was the producer of this biographical film.  |\n",
    "| 22 | 0.714286 | Similar | PREPROCESSED | godbout v longueuil ( city of ) | longueuil | ['godbout v. longueuil', 'godbout v.', 'v. longueuil', 'the godbout v. longueuil', 'godbout v. longueuil case', 'godbout', 'longueuil'] | what court handled the godbout v. longueuil case? |\n",
    "| 23 | 0.875 | Similar | PREPROCESSED | raymond a. meier | raymond | ['raymond meier', 'meier', 'was raymond meier', 'raymond'] | Which city was raymond meier born in |\n",
    "| 24 | 0.714286 | Wrong Span | PREPROCESSED | afterglow | the afterglow | ['the afterglow fil', 'the afterglow fil ,', 'afterglow fil', 'afterglow fil ,', 'the afterglow'] | who did the music for the afterglow fil, |\n",
    "| 25 | 0.0 | Wrong Span | PREPROCESSED | album | leo sayer | ['leo sayer'] | what is an album by leo sayer? |\n",
    "| 26 | 1.0 | Similar | PREPROCESSED | drums | drum | ['drum'] | who played the drum  in the Los Angeles rock quintet Rooney |\n",
    "| 27 | 0.625 | Wrong Span | PREPROCESSED | latin pop | pop music | ['latin pop music', 'pop music'] | who is an artist that creates latin pop music |\n",
    "| 28 | 0.0909091 | Wrong Span | PREPROCESSED | death eaters | harry potter | ['harry potter'] | which is the name of a death eater in harry potter? |\n",
    "| 29 | 0.62069 | Similar | NORMALIZED_PUNCTUATION | single - player video game | single - | ['single - player mode', 'single - player mode game', 'single - player', 'single -'] | what is a single-player mode game? |\n",
    "| 30 | 0.461538 | Wrong Span | PREPROCESSED | album | compilation album | ['compilation album'] | What is a compilation album from 2006  |\n",
    "| 31 | 0.047619 | Wrong Span | PREPROCESSED | studio album | arcangel | ['arcangel'] | What was a studio album recording for Arcángel |\n",
    "| 32 | 0.6 | Wrong Span | PREPROCESSED | saint | the saint | ['the saint novel', 'saint novel', 'the saint'] | What type of book is the saint novel? |\n",
    "| 33 | 0.5 | Wrong Span | PREPROCESSED | avila place | avila | ['avila'] | what western state does contain avila place |\n",
    "| 34 | 0.0357143 | Wrong Span | PREPROCESSED | north carolina | surrey | ['surrey county', 'surrey'] | What is a city in Surrey County, north carolina? |\n",
    "| 35 | 0.0 | Wrong Span | PREPROCESSED | album | johannes brahms | ['johannes brahms'] | What is an album written by Johannes Brahms |\n",
    "| 36 | 0.777778 | Wrong Span | PREPROCESSED | first battle of james island | james island | ['battle of james island', 'battle of james', 'of james island', 'james island'] | Name a soldier involved in the battle of james island. |\n",
    "| 37 | 0.642857 | Wrong Span | PREPROCESSED | plymouth | plymouth rock | ['plymouth rock'] | is there another attraction in plymouth other than plymouth rock |\n",
    "| 38 | 0.705882 | Wrong Span | PREPROCESSED | altered beast | beast | ['beast game', 'altered beast game', 'beast'] | who is the creator of the altered beast game |\n",
    "| 39 | 0.45 | Wrong Span | PREPROCESSED | the barefoot artist | barefoot | ['barefoot'] | which film created the barefoot artist  |\n",
    "| 40 | 0.352941 | Wrong Span | PREPROCESSED | pornographic actor | actor | ['actor'] | who is a pornographic actor |\n",
    "| 41 | 0.0 | Wrong Span | PREPROCESSED | album | century media | ['century media'] | which albums were released by the century media label? |\n",
    "| 42 | 0.5 | Wrong Span | PREPROCESSED | 8833 acer | acer | ['acer'] | what is a 8833 acer |\n",
    "| 43 | 1.0 | Similar | PREPROCESSED | cruisin ' | cruisin | ['cruisin'] | What release is cruisin on? |\n",
    "| 44 | 0.88 | Similar | NORMALIZED_PUNCTUATION_STEM | the wonderful wizard of ha 's | the wonderful | ['the wonderful wizard of', 'the wonderful wizard of has', 'the wonderful wizard', 'wonderful wizard of', 'wonderful wizard of has', 'wonderful wizard', 'wizard of', 'the wonderful'] | What film series is the wonderful wizard of has from? |\n",
    "| 45 | 0.842105 | Wrong Span, Article | PREPROCESSED | tower of london | the tower of london | ['the tower of london'] | who recorded the tower of london |\n",
    "| 46 | 0.7 | Wrong Span | PREPROCESSED | outside in | outside | ['outside'] | Which genre is outside in associated with |\n",
    "| 47 | 0.4 | Wrong Span | PREPROCESSED | rca | rca records | ['rca records'] | Who is an artist  signed by rca records? |\n",
    "| 48 | 0.0 | Wrong Span | PREPROCESSED | action game | sega | ['sega'] | What's an action game made by sega |\n",
    "| 49 | 0.647059 | Wrong Span | PREPROCESSED | film adaptation | novel | ['novel film adaptation', 'novel'] | What's an example of a novel film adaptation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3\n",
    "\n",
    "In version 2, the error bucketing revealed a failure to handling similar aliases's; therefore, we proceed to handle them in version 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=65536)\n",
    "def cached_similar_alias_normalized_punctuation_stem_to_alias(text, limit):\n",
    "    cursor.execute(\"\"\"SELECT set_limit(\"\"\" + str(limit) + \"\"\");\n",
    "                    SELECT DISTINCT alias FROM fb_two_subject_name \n",
    "                    WHERE alias_normalized_punctuation_stem %% %s\"\"\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528d0fede4894f29a054c1ec1b8449ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answerable Precision: 0.972956 [20615 of 21188]\n",
      "Answerable Recall: 1.000000 [21188 of 21188]\n",
      "Expected Guessing Accuracy: 0.645286 [13942 of 21607]\n",
      "Average Number of Aliases: 1.0374740419105153\n"
     ]
    }
   ],
   "source": [
    "from lib.table import format_pipe_table\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    limit = 0.8\n",
    "    while limit > 0:\n",
    "        is_break = False\n",
    "        for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "            candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "\n",
    "            # Punctuation Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                    text_normalize_punctuation(predicted['name']))\n",
    "\n",
    "            # Suffix Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']))\n",
    "\n",
    "            # Other Similar Aliases\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']), limit)\n",
    "\n",
    "            if len(candidate_aliases) > 0:\n",
    "                candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "                n_aliases += len(candidate_aliases)\n",
    "                is_break = True\n",
    "                break\n",
    "        if is_break:\n",
    "            break\n",
    "        limit -= 0.1\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average Number of Aliases:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "    \n",
    "Version 3\n",
    "- Precision: 0.974171 [10372 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
    "- Average Number Of Aliases: 1.0379413974455296\n",
    "\n",
    "Recall stayed the same.\n",
    "Precision increased by 0.000751.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "The increase here is small; therefore, it may not be worth it to include this last step in the pipeline. Without this last step, there is little room to grow otherwise with SQL queries. \"Average Number of Aliases\" does indicate that there is some room to grow in filtering out aliases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4\n",
    "\n",
    "In Version 4, we investigate alias filtering via mean candidate distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51feef28d5d54418a3260d32d3002952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.831220 [8850 of 10647]\n",
      "Recall: 0.999906 [10647 of 10648]\n",
      "Expected Guessing Accuracy: 0.671872 [7154 of 10648]\n",
      "Average number of alaises: 1.000939143501127\n"
     ]
    }
   ],
   "source": [
    "# TODO: Consider picking the alias that on average is closest to the candidate. The average summing over all\n",
    "# aliases for that MID.\n",
    "\n",
    "from Levenshtein import distance\n",
    "from lib.table import format_pipe_table\n",
    "import statistics\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']), 0.8)\n",
    "\n",
    "        if len(candidate_aliases) > 0:\n",
    "            # Filter by smallest edit distance to originally predicted name\n",
    "            score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "            best_score = min([score(a) for a in candidate_aliases])\n",
    "            candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "            \n",
    "            # Copute the number of aliases\n",
    "            n_aliases += len(candidate_aliases)\n",
    "            mids = cached_aliases_to_mids(candidate_aliases)\n",
    "            scores = []\n",
    "            # IF there exists more aliases for a mid, we average\n",
    "            for mid in mids:\n",
    "                cursor.execute('SELECT alias FROM fb_two_name WHERE mid = %s', (mid,))\n",
    "                score = statistics.mean([distance(r[0], predicted['name']) for r in cursor.fetchall()])\n",
    "                scores.append(score)\n",
    "            min_score = min(scores)\n",
    "            mids = [mid for i, mid in enumerate(mids) if scores[i] == min_score]\n",
    "            candidates_mids.append(mids)\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average number of alaises:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 5\n",
    "\n",
    "In Version 5, we investigate alias filtering via edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider picking the alias that on average is closest to the candidate. The average summing over all\n",
    "# aliases for that MID.\n",
    "# TODO: Consider picking the alias with the largest amount of aggragate object mids\n",
    "\n",
    "from Levenshtein import distance\n",
    "from lib.table import format_pipe_table\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']), 0.8)\n",
    "\n",
    "        if len(candidate_aliases) > 0:\n",
    "            # Filter by smallest edit distance to originally predicted name\n",
    "            score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "            best_score = min([score(a) for a in candidate_aliases])\n",
    "            candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "            \n",
    "            # Copute the number of aliases\n",
    "            n_aliases += len(candidate_aliases)\n",
    "            \n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average number of alaises:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "    \n",
    "Version 3\n",
    "- Precision: 0.974171 [10372 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
    "- Average Number Of Aliases: 1.0379413974455296\n",
    "\n",
    "Version 5\n",
    "- Precision: 0.972387 [10353 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.676108 [7199 of 10648]\n",
    "- Average number of alaises: 1.000939143501127\n",
    "\n",
    "Recall stayed the same.\n",
    "Precision decreased by 0.001784.\n",
    "The expected accuracy went up 0.00655 by close to half a percent.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "With a small decrease in percision, we were able to reduce the number of aliases to choose from. Resulting in a 0.65% increase in our expected accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin\n",
    "\n",
    "Here we use our algorithm to generate candidates and save the results of Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "from numpy import nan\n",
    "\n",
    "def generate_candidates(cursor, row):\n",
    "    limit = 0.85\n",
    "    while limit > 0:\n",
    "        for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "            candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "            \n",
    "            # Punctuation Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                    text_normalize_punctuation(predicted['name']))\n",
    "\n",
    "            # Suffix Differences\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']))\n",
    "\n",
    "            # Other Similar Aliases\n",
    "            if len(candidate_aliases) == 0:\n",
    "                candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                    text_normalize_punctuation_stem(predicted['name']), limit)\n",
    "\n",
    "            if len(candidate_aliases) > 0:\n",
    "                # Filter by smallest edit distance to originally predicted name\n",
    "                # TODO: Look into filtering after the relation filter\n",
    "                score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "                best_score = min([score(a) for a in candidate_aliases])\n",
    "                candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "                mids = cached_aliases_to_mids(candidate_aliases)\n",
    "                row['candidate_mids'] = mids\n",
    "                row['predicted_start_index'] = predicted['start_index']\n",
    "                row['predicted_end_index'] = predicted['end_index']\n",
    "                row['predicted_subject_name'] = predicted['name']\n",
    "                return row\n",
    "        limit -= 0.1\n",
    "        \n",
    "    row['candidate_mids'] = []\n",
    "    row['predicted_start_index'] = nan\n",
    "    row['predicted_end_index'] = nan\n",
    "    row['predicted_subject_name'] = nan\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36bd86a861449a7817aa03387d9c835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21687), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "df = df.progress_apply(partial(generate_candidates, cursor), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity\n",
    "\n",
    "Check if `generate_candidates` works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc439284753420589f6ef41a205496a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21687), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Accuracy: 0.953521 [20679 of 21687]\n",
      "Expected Accuracy: 0.652421 [14149 of 21687]\n",
      "Subject Name Accuracy: 0.922119 [19998 of 21687]\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy just to check the implementation of `generate_candidates`.\n",
    "correct = 0\n",
    "expected_correct = 0\n",
    "subject_name_correct = 0\n",
    "for index, row in tqdm_notebook(df.iterrows(), total=df.shape[0]):\n",
    "    if row['subject'] in row['candidate_mids']:\n",
    "        correct += 1\n",
    "        expected_correct += 1 / len(row['candidate_mids'])\n",
    "    \n",
    "    if (isinstance(row['subject_name'], str) and\n",
    "        text_preprocess(row['subject_name']) == row['predicted_subject_name']):\n",
    "        subject_name_correct += 1\n",
    "        \n",
    "print('Candidate Accuracy: %f [%d of %d]' % (correct / df.shape[0], correct, df.shape[0]))\n",
    "print('Expected Accuracy: %f [%d of %d]' % (expected_correct / df.shape[0], expected_correct, df.shape[0]))\n",
    "# TODO: Look at subject names that are incorrect but the subject is correct\n",
    "# Because that's weird.\n",
    "print('Subject Name Accuracy: %f [%d of %d]' %\n",
    "      (subject_name_correct / df.shape[0],subject_name_correct, df.shape[0]))\n",
    "\n",
    "# Candidate Accuracy: 0.951169 [20628 of 21687]\n",
    "# Expected Accuracy: 0.650218 [14101 of 21687]\n",
    "# Subject Name Accuracy: 0.928252 [20131 of 21687]\n",
    "\n",
    "# Candidate Accuracy: 0.953521 [20679 of 21687]\n",
    "# Expected Accuracy: 0.652421 [14149 of 21687]\n",
    "# Subject Name Accuracy: 0.922119 [19998 of 21687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Pipeline\n",
    "\n",
    "Write step 2 results to use them in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('step_2_generate_candidates.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
