{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Recognition CRF Tagger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../allennlp')\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from allennlp.common.params import Params\n",
    "\n",
    "crf_tagger = {\n",
    "    \"dataset_reader\": {\n",
    "        \"type\": \"sequence_tagging\",\n",
    "        \"word_tag_delimiter\": \"/\",\n",
    "        \"token_indexers\": {\n",
    "            \"tokens\": {\n",
    "                \"type\": \"single_id\",\n",
    "                \"lowercase_tokens\": True\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"characters\",\n",
    "                \"character_tokenizer\": {\n",
    "                    \"end_tokens\": [\"@@PADDING@@\", \"@@PADDING@@\", \"@@PADDING@@\", \"@@PADDING@@\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"pytorch_seed\": 4006490763, # 0.957081 Other random seeds ranged from 0.954 - 0.956\n",
    "    \"numpy_seed\": 4006490763,\n",
    "    \"random_seed\": 4006490763,\n",
    "    # TODO: Update this to the location for subject name recognition training data\n",
    "    \"train_data_path\": './../../data/subject_recognition/train.txt',\n",
    "    \"validation_data_path\": './../../data/subject_recognition/dev.txt',\n",
    "    \"model\": {\n",
    "        \"type\": \"crf_tagger\",\n",
    "        \"text_field_embedder\": {\n",
    "            \"tokens\": {\n",
    "                \"type\":\n",
    "                    \"embedding\",\n",
    "                \"pretrained_file\":\n",
    "                    \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "                \"embedding_dim\":\n",
    "                    100,\n",
    "                \"trainable\":\n",
    "                    False\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 25\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 25,\n",
    "                    \"num_filters\": 100,\n",
    "                    \"ngram_filter_sizes\": [5]\n",
    "                },\n",
    "                \"dropout\": 0.25\n",
    "            }\n",
    "        },\n",
    "        \"encoder\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"input_size\": 200,\n",
    "            \"hidden_size\": 600,\n",
    "            \"num_layers\": 3,\n",
    "            \"dropout\": 0.25,\n",
    "            \"bidirectional\": True\n",
    "        },\n",
    "        \"regularizer\": [[\"transitions$\", {\"type\": \"l2\", \"alpha\": 0.01}]],\n",
    "    },\n",
    "    \"iterator\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"sorting_keys\": [[\"tokens\", \"num_tokens\"]],\n",
    "        \"padding_noise\": 0.3,\n",
    "        \"batch_size\": 64,\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"num_epochs\": 100,\n",
    "        \"cuda_device\": 0,\n",
    "        \"patience\": 6,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"adam\",\n",
    "            \"amsgrad\": False,\n",
    "        },\n",
    "        \"validation_metric\": \"+accuracy\",\n",
    "        \"learning_rate_scheduler\":  {\n",
    "          \"type\": \"reduce_on_plateau\",\n",
    "          \"factor\": 0.5,\n",
    "          \"mode\": \"max\",\n",
    "          \"patience\": 2,\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.utils import config_logging\n",
    "# from lib.utils import new_experiment_folder\n",
    "# from allennlp.commands.train import train_model\n",
    "\n",
    "# config_logging()\n",
    "# experiment_folder = new_experiment_folder(label='subject_recognition', parent_directory='../../experiments/')\n",
    "# # experiment_folder = '../../experiments/subject_recognition.02_11_08:21:07'\n",
    "# params = Params(crf_tagger)\n",
    "# print('Serialization Directory:', experiment_folder)\n",
    "# train_model(params=params, serialization_dir=experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import itertools\n",
    "\n",
    "token_characters_ = []\n",
    "# base_rnn_token_characters = {\n",
    "#     \"type\": \"character_encoding\",\n",
    "#     \"embedding\": {\n",
    "#         \"embedding_dim\": 25\n",
    "#     },\n",
    "#     \"encoder\": {\n",
    "#         \"type\": \"gru\",\n",
    "#         \"input_size\": 25,\n",
    "#         \"hidden_size\": 50,\n",
    "#         \"num_layers\": 2,\n",
    "#         \"dropout\": 0.25,\n",
    "#         \"bidirectional\": True\n",
    "#     },\n",
    "#     \"dropout\": 0.25,\n",
    "# }\n",
    "# rnn_token_characters_space = [[0.0, 0.25, 0.5], [1, 2, 3], ['gru', 'lstm']]\n",
    "# for dropout, num_layers, type_ in list(itertools.product(*rnn_token_characters_space)):\n",
    "#     copy = deepcopy(base_rnn_token_characters)\n",
    "#     copy['encoder']['dropout'] = dropout\n",
    "#     copy['dropout'] = dropout\n",
    "#     copy['encoder']['num_layers'] = num_layers\n",
    "#     copy['encoder']['type'] = type_\n",
    "#     token_characters_.append(copy)\n",
    "\n",
    "base_cnn_token_characters = {\n",
    "    \"type\": \"character_encoding\",\n",
    "    \"embedding\": {\n",
    "        \"embedding_dim\": 25\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"cnn\",\n",
    "        \"embedding_dim\": 25,\n",
    "        \"num_filters\": 100,\n",
    "        \"ngram_filter_sizes\": [5]\n",
    "    },\n",
    "    \"dropout\": 0.25,\n",
    "}\n",
    "cnn_token_characters_space = [[0.25]]\n",
    "for dropout, in list(itertools.product(*cnn_token_characters_space)):\n",
    "    copy = deepcopy(base_cnn_token_characters)\n",
    "    copy['dropout'] = dropout\n",
    "    token_characters_.append(copy)\n",
    "\n",
    "encoders = []\n",
    "base_encoder = {\n",
    "    \"type\": \"lstm\",\n",
    "    \"input_size\": 200,\n",
    "    \"hidden_size\": 200,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.25,\n",
    "    \"bidirectional\": True\n",
    "}\n",
    "encoder_space = [[0.25], [2, 3, 4, 5], [600, 800, 1000], ['lstm']]\n",
    "encoder_points = list(itertools.product(*encoder_space))\n",
    "for dropout, num_layers, hidden_size, type_ in encoder_points:\n",
    "    copy = deepcopy(base_encoder)\n",
    "    copy['dropout'] = dropout\n",
    "    copy['num_layers'] = num_layers\n",
    "    copy['hidden_size'] = hidden_size\n",
    "    copy['type'] = type_\n",
    "    encoders.append(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points: 32\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "space = [[random.randint(0, 2**32) for i in range(32)]]\n",
    "points = list(itertools.product(*space))\n",
    "random.shuffle(points)\n",
    "print('Number of points: %d' % len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.utils import config_logging\n",
    "from lib.utils import new_experiment_folder\n",
    "from allennlp.commands.train import train_model\n",
    "import json\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import os, sys\n",
    "import stat\n",
    "\n",
    "#config_logging()\n",
    "\n",
    "for i, (rand_int,) in enumerate(points):\n",
    "    hyperparameters = copy.deepcopy(crf_tagger)\n",
    "    hyperparameters['pytorch_seed'] = rand_int\n",
    "    hyperparameters['numpy_seed'] = rand_int\n",
    "    hyperparameters['random_seed'] = rand_int\n",
    "    print('â€“' * 100)\n",
    "    experiment_folder = new_experiment_folder(label='subject_recognition_grid_search_' + str(i),\n",
    "                                          parent_directory='../../experiments/')\n",
    "    print('Seed: %s' % rand_int)\n",
    "    print('Serialization Directory:', experiment_folder)\n",
    "    params = Params(hyperparameters)\n",
    "    train_model(params=params, serialization_dir=experiment_folder)\n",
    "    shutil.rmtree(experiment_folder,ignore_errors=True)\n",
    "    params = None\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
